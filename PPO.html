<!DOCTYPE html><html lang="en,zh-CN"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="codeva-X99uCprQdB"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="choucisan"><meta name="keywords" content=""><meta name="description" content="引言： 近端策略优化（PPO）是一种新的强化学习策略梯度方法，该方法通过与环境交互采样数据和使用随机梯度上升优化“替代”目标函数来交替进行。标准的策略梯度方法每个数据样本执行一次梯度更新，PPO提出了一种新的目标函数能够实现多轮小批量更新，它具备信任区域策略优化（TRPO）的部分优势，但实现起来更为简单、适用性更广，并且（从经验上看）具有更好的样本复杂度。实验结果表明PP"><meta property="og:type" content="article"><meta property="og:title" content="PPO——近端策略优化算法"><meta property="og:url" content="https://choucisan.xyz/PPO.html"><meta property="og:site_name" content="choucisan&#39;s blog"><meta property="og:description" content="引言： 近端策略优化（PPO）是一种新的强化学习策略梯度方法，该方法通过与环境交互采样数据和使用随机梯度上升优化“替代”目标函数来交替进行。标准的策略梯度方法每个数据样本执行一次梯度更新，PPO提出了一种新的目标函数能够实现多轮小批量更新，它具备信任区域策略优化（TRPO）的部分优势，但实现起来更为简单、适用性更广，并且（从经验上看）具有更好的样本复杂度。实验结果表明PP"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://images.unsplash.com/photo-1742382717882-3cfb6971f44b?q=80&w=2069&auto=format&fit=crop"><meta property="article:published_time" content="2025-12-15T03:00:26.000Z"><meta property="article:modified_time" content="2025-12-12T06:30:39.317Z"><meta property="article:author" content="choucisan"><meta property="article:tag" content="Research"><meta property="article:tag" content="Algorithm"><meta property="article:tag" content="PPO"><meta property="article:tag" content="OpenAI"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://images.unsplash.com/photo-1742382717882-3cfb6971f44b?q=80&w=2069&auto=format&fit=crop"><title>PPO——近端策略优化算法 - choucisan&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link rel="stylesheet" href="/css/fullscreen-style.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"choucisan.xyz",root:"/",version:"1.9.8",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"VnqS06x2hxlj7MEjoUAix6L5-gzGzoHsz",app_key:"ck0fCo8cxJs0OdTEdWmSl9l1",server_url:"https://vnqs06x2.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2025-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>13log</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><span>HOME</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><span>ARCHIVES</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>TOPICS</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="/categories/Pub/" target="_self"><span>Publications</span> </a><a class="dropdown-item" href="/categories/OpenSource/" target="_self"><span>Open Source</span> </a><a class="dropdown-item" href="/categories/Paper/" target="_self"><span>Paper Reading</span> </a><a class="dropdown-item" href="/categories/Tech/" target="_self"><span>Tech Share</span> </a><a class="dropdown-item" href="/categories/Life/" target="_self"><span>Life Share</span></a></div></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><span>ABOUT</span></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">PPO——近端策略优化算法</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-12-15 11:00" pubdate>December 15, 2025 am</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.9k words </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 33 mins </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> views</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">PPO——近端策略优化算法</h1><div class="markdown-body"><div class="note note-info"><p><strong>引言：</strong> 近端策略优化（PPO）是一种新的强化学习策略梯度方法，该方法通过与环境交互采样数据和使用随机梯度上升优化“替代”目标函数来交替进行。标准的策略梯度方法每个数据样本执行一次梯度更新，PPO提出了一种新的目标函数能够实现多轮小批量更新，它具备信任区域策略优化（TRPO）的部分优势，但实现起来更为简单、适用性更广，并且（从经验上看）具有更好的样本复杂度。实验结果表明PPO优于其他在线策略梯度方法，并且在样本复杂度、简便性和实际运行时间之间取得了良好的平衡。</p></div><h2 id="ppo算法介绍">✈️ PPO算法介绍</h2><h3 id="算法归类">算法归类</h3><p>PPO 是目前强化学习领域最主流的算法之一，它在分类谱系中占据着核心位置：</p><ul><li><strong>基于策略 (Policy-Based)</strong>：PPO 的核心是直接优化策略 <span class="math inline"><em>π</em>(<em>a</em>|<em>s</em>)</span>，而不是像 DQN 那样间接优化价值 Q。</li><li><strong>在线策略 (On-Policy)</strong>：PPO 是一种 On-Policy 算法。这意味着“采样的策略”和“学习的策略”必须是同一个。它不能像 DQN 那样使用经验回放池（Replay Buffer）里的过期数据，因此每次更新完策略后，旧的数据就必须丢弃。</li><li><strong>演员-评论家框架 (Actor-Critic)</strong>：PPO 通常结合 Actor-Critic 架构。Actor 负责决策，Critic 负责打分（估计优势函数），两者相辅相成。</li></ul><h3 id="适用范围与局限">适用范围与局限</h3><ul><li><strong>适用范围</strong>：<ul><li><strong>连续与离散动作空间</strong>：PPO 的最大优势之一是通用性。它既可以玩《超级马里奥》（离散动作），也可以控制机器人手臂抓取物体（连续动作）。</li><li><strong>复杂环境</strong>：在许多高维、复杂的环境中（如 OpenAI Five 打 Dota2），PPO 表现出了极强的稳定性。</li></ul></li><li><strong>局限性</strong>：<ul><li><strong>样本效率较低</strong>：由于是 On-Policy 算法，数据用完即弃，相比于 Off-Policy 算法（如 SAC, TD3），PPO 需要更多的交互数据才能收敛。</li><li><strong>探索能力受限</strong>：PPO 倾向于在已知的安全区域内优化，对于需要深度探索（稀疏奖励）的任务，可能容易陷入局部最优。</li></ul></li></ul><h3 id="基本思想">基本思想</h3><p>PPO 的核心思想可以概括为：<strong>“步子迈小点，走得稳一点”</strong>。</p><p>在传统的策略梯度（Policy Gradient）中，如果学习率设置得太大，策略更新幅度过猛，很容易导致模型性能“断崖式下跌”，而且很难恢复。 TRPO（信任区域策略优化）试图通过复杂的数学约束（KL 散度）来限制更新幅度，但计算量太大。</p><p>PPO 继承了 TRPO 的思想，但用了一种<strong>更简单的工程方法（截断/Clipping）</strong>来实现： 我们在目标函数里强行规定，新策略 <span class="math inline"><em>π</em><sub><em>n</em><em>e</em><em>w</em></sub></span> 和旧策略 <span class="math inline"><em>π</em><sub><em>o</em><em>l</em><em>d</em></sub></span> 的差异不能太大。如果差异太大，就不给奖励，甚至给惩罚。这保证了每一次更新都是在“信任区域”内的安全提升。</p><hr><h2 id="ppo算法流程">🚀 PPO算法流程</h2><p>PPO 的强大来自于三个核心组件的有机结合：GAE 用于准确估计优势，KL 或 CLIP 用于限制更新幅度。</p><figure><img src="/images/rl/PPO.png" srcset="/img/loading.gif" lazyload alt="PPO算法流程"><figcaption aria-hidden="true">PPO算法流程</figcaption></figure><h3 id="优势函数-gae-generalized-advantage-estimation">优势函数 GAE (Generalized Advantage Estimation)</h3><p>在策略梯度中，我们需要评估一个动作“好不好”，这需要用到<strong>优势函数 (Advantage Function)</strong> <span class="math inline"><em>A</em>(<em>s</em>, <em>a</em>)</span>。 如果直接用蒙特卡洛回报 <span class="math inline"><em>G</em><sub><em>t</em></sub></span> 减去基线 <span class="math inline"><em>V</em>(<em>s</em>)</span>，方差会很大；如果用 TD 差分，偏差会很大。</p><p><strong>GAE</strong> 提出了一种在偏差和方差之间做权衡的方法。它定义了一个指数加权的优势估计：</p><p><span class="math display">$$ A_t^{GAE} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} $$</span></p><p>其中 <span class="math inline"><em>δ</em><sub><em>t</em></sub> = <em>r</em><sub><em>t</em></sub> + <em>γ</em><em>V</em>(<em>s</em><sub><em>t</em> + 1</sub>) − <em>V</em>(<em>s</em><sub><em>t</em></sub>)</span> 是单步的 TD Error。</p><ul><li><span class="math inline"><em>λ</em> = 0</span>：GAE 变成了普通的 TD Error（偏差大，方差小）。</li><li><span class="math inline"><em>λ</em> = 1</span>：GAE 变成了蒙特卡洛估计（偏差小，方差大）。</li><li><strong>PPO 的选择</strong>：通常取 <span class="math inline"><em>λ</em> ≈ 0.95</span>，在两者之间取得最佳平衡。</li></ul><h3 id="替代目标函数与概率比率-surrogate-objective">替代目标函数与概率比率 (Surrogate Objective)</h3><p>为了能够利用旧策略 <span class="math inline"><em>π</em><sub><em>θ</em><sub><em>o</em><em>l</em><em>d</em></sub></sub></span> 采样的数据来更新当前的新策略 <span class="math inline"><em>π</em><sub><em>θ</em></sub></span>（即实现多轮小批量更新），PPO 引入了<strong>重要性采样 (Importance Sampling)</strong> 的技巧。</p><p>我们需要定义新旧策略在动作概率上的<strong>比率 (Probability Ratio)</strong>：</p><p><span class="math display">$$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$</span></p><p>基于这个比率，我们可以构建一个最基础的<strong>替代</strong>目标函数（Conservative Policy Iteration, CPI）：</p><p><span class="math display">$$ L^{CPI}(\theta) = \mathbb{E}_t \left[ r_t(\theta) A_t \right] = \mathbb{E}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} A_t \right] $$</span></p><p><strong>公式解析：</strong></p><ul><li><strong><span class="math inline"><em>r</em><sub><em>t</em></sub>(<em>θ</em>)</span></strong>：衡量了新策略相对于旧策略的变化幅度。<ul><li>如果 <span class="math inline"><em>r</em><sub><em>t</em></sub> = 1</span>，说明新旧策略一样。</li><li>如果 <span class="math inline"><em>r</em><sub><em>t</em></sub> &gt; 1</span>，说明新策略比旧策略更倾向于选择该动作。</li></ul></li><li><strong>优化逻辑</strong>：<ul><li>当动作优势 <strong><span class="math inline"><em>A</em><sub><em>t</em></sub> &gt; 0</span></strong>（好动作）时，最大化目标函数会促使 <span class="math inline"><em>r</em><sub><em>t</em></sub></span> 变大，即<strong>增加</strong>该动作的概率。</li><li>当动作优势 <strong><span class="math inline"><em>A</em><sub><em>t</em></sub> &lt; 0</span></strong>（坏动作）时，最大化目标函数会促使 <span class="math inline"><em>r</em><sub><em>t</em></sub></span> 变小，即<strong>抑制</strong>该动作的概率。</li></ul></li></ul><p><strong>存在的问题：</strong> 如果没有约束，直接最大化 <span class="math inline"><em>L</em><sup><em>C</em><em>P</em><em>I</em></sup></span> 会导致 <span class="math inline"><em>r</em><sub><em>t</em></sub></span> 发生剧烈变化（例如为了追求高分，把某个好动作的概率瞬间提得非常高）。这会导致新策略 <span class="math inline"><em>π</em><sub><em>θ</em></sub></span> 严重偏离旧策略，破坏训练的稳定性。因此，我们需要接下来的 <strong>CLIP 剪切</strong> 机制来限制它。</p><h3 id="剪切-clip-ppo-clip">剪切 CLIP (PPO-Clip)</h3><p>这是 PPO 的第二种变体，也是<strong>目前最流行、最常用的版本</strong>。它不需要计算复杂的 KL 散度，而是直接用 <code>min</code> 和 <code>clip</code> 函数来锁死更新幅度。</p><p>定义概率比率 <span class="math inline">$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</span>。PPO-Clip 的目标函数为：</p><p><span class="math display"><em>L</em><sup><em>C</em><em>L</em><em>I</em><em>P</em></sup>(<em>θ</em>) = 𝔼<sub><em>t</em></sub>[min (<em>r</em><sub><em>t</em></sub>(<em>θ</em>)<em>A</em><sub><em>t</em></sub>, clip(<em>r</em><sub><em>t</em></sub>(<em>θ</em>), 1 − <em>ϵ</em>, 1 + <em>ϵ</em>)<em>A</em><sub><em>t</em></sub>)]</span></p><p><strong>公式解析</strong>：</p><ul><li><strong><span class="math inline"><em>r</em><sub><em>t</em></sub>(<em>θ</em>)<em>A</em><sub><em>t</em></sub></span></strong>：这是原始的策略梯度目标。我们希望 <span class="math inline"><em>A</em><sub><em>t</em></sub> &gt; 0</span> 时 <span class="math inline"><em>r</em><sub><em>t</em></sub></span> 变大（增加好动作的概率）。</li><li><strong><span class="math inline">clip(..., 1 − <em>ϵ</em>, 1 + <em>ϵ</em>)</span></strong>：强制将比率限制在 <span class="math inline">[0.8, 1.2]</span> 之间（假设 <span class="math inline"><em>ϵ</em> = 0.2</span>）。</li><li><strong><span class="math inline">min </span>操作</strong>：这是一个悲观的下界估计。<ul><li>当动作是<strong>好的 (<span class="math inline"><em>A</em><sub><em>t</em></sub> &gt; 0</span>)</strong>：如果模型概率提升太多（超过 1.2 倍），目标函数就被截断，不再给予奖励。这防止了“过度自信”。</li><li>当动作是<strong>坏的 (<span class="math inline"><em>A</em><sub><em>t</em></sub> &lt; 0</span>)</strong>：如果模型概率降低太多（低于 0.8 倍），目标函数也被截断。这防止了“过度恐慌”导致策略坍塌。</li></ul></li></ul><p>通过这种简单粗暴的<strong>“截断”</strong>机制，PPO 极其高效地实现了策略的平稳更新。</p><h2 id="损失函数与优化目标">⚖️ 损失函数与优化目标</h2><p>仅仅依靠 CLIP 剪切目标只能优化策略网络（Actor），但在 PPO 的实际应用中（通常采用 Actor-Critic 架构），我们需要构建一个复合的<strong>总目标函数 (Total Objective Function)</strong>，以便同时完成策略改进、价值函数拟合以及鼓励探索这三大任务。</p><p>PPO 的最终优化目标 <span class="math inline"><em>L</em><sub><em>t</em></sub><sup><em>T</em><em>o</em><em>t</em><em>a</em><em>l</em></sup>(<em>θ</em>)</span> 由三部分组成：</p><p><span class="math display">$$ L_t^{Total}(\theta) = \underbrace{L_t^{CLIP}(\theta)}_{\text{策略目标}} - \underbrace{c_1 L_t^{VF}(\theta)}_{\text{价值损失}} + \underbrace{c_2 S[\pi_\theta](s_t)}_{\text{熵正则化}} $$</span></p><p>我们通过<strong>最大化</strong>这个总目标函数来更新网络参数（或者通过最小化其负值）。</p><h3 id="策略目标-policy-objective">策略目标 (Policy Objective)</h3><p>即前文提到的剪切目标 <span class="math inline"><em>L</em><sub><em>t</em></sub><sup><em>C</em><em>L</em><em>I</em><em>P</em></sup></span>。这是核心部分，用于提升策略的表现，同时确保更新幅度在安全范围内。</p><h3 id="价值函数损失-value-function-loss">价值函数损失 (Value Function Loss)</h3><p>为了计算优势函数 <span class="math inline"><em>A</em><sub><em>t</em></sub></span>（用于指导策略更新），我们需要一个 Critic 网络来估计状态价值 <span class="math inline"><em>V</em>(<em>s</em>)</span>。Critic 越准，优势估计就越准。 因此，我们需要最小化价值网络的预测误差。通常使用均方误差 (MSE)：</p><p><span class="math display"><em>L</em><sub><em>t</em></sub><sup><em>V</em><em>F</em></sup>(<em>θ</em>) = (<em>V</em><sub><em>θ</em></sub>(<em>s</em><sub><em>t</em></sub>) − <em>V</em><sub><em>t</em></sub><sup><em>t</em><em>a</em><em>r</em><em>g</em><em>e</em><em>t</em></sup>)<sup>2</sup></span></p><ul><li><strong><span class="math inline"><em>V</em><sub><em>θ</em></sub>(<em>s</em><sub><em>t</em></sub>)</span></strong>：当前网络预测的状态价值。</li><li><strong><span class="math inline"><em>V</em><sub><em>t</em></sub><sup><em>t</em><em>a</em><em>r</em><em>g</em><em>e</em><em>t</em></sup></span></strong>：真实的价值目标（通常由计算出的回报 <span class="math inline"><em>G</em><sub><em>t</em></sub></span> 或 GAE 推算出的价值替代）。</li><li><strong><span class="math inline"><em>c</em><sub>1</sub></span></strong>：价值损失系数（通常取 0.5），用于平衡策略优化和价值拟合的权重。</li><li><em>注：公式中的负号表示我们要最大化总目标，即等同于最小化价值误差。</em></li></ul><div class="note note-primary"><p>PPO 的价值函数更新通常被视为一个回归问题。与 DQN 不同，PPO 由于是 On-Policy 算法，其价值目标是基于当次交互轨迹计算得出的固定回报（如 GAE），因此不需要使用额外的目标网络（Target Network）或软更新机制</p></div><h3 id="熵正则化-entropy-bonus">熵正则化 (Entropy Bonus)</h3><p>在强化学习中，过早收敛（Premature Convergence）是一个常见问题，即智能体在还没探索完环境时就认定某种次优策略是最好的，从而不再尝试其他可能。 为了解决这个问题，我们在目标函数中加入<strong>策略熵 (Entropy)</strong> 项 <span class="math inline"><em>S</em></span>。熵越大，代表策略越随机（探索性越强）；熵越小，代表策略越确定。</p><p><span class="math display"><em>S</em>[<em>π</em><sub><em>θ</em></sub>](<em>s</em><sub><em>t</em></sub>) = −∑<em>a</em><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em><sub><em>t</em></sub>)log <em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em><sub><em>t</em></sub>)</span></p><ul><li><strong><span class="math inline"><em>c</em><sub>2</sub></span></strong>：熵系数（通常取 0.01 左右）。</li><li><strong>作用</strong>：作为一个“奖励”项，它鼓励模型在训练初期保持一定的随机性，多去探索未知的动作，防止策略过早塌缩成确定性策略。</li></ul><h3 id="总结">总结</h3><p>最终，PPO 实际上是在做三件事的平衡：</p><ul><li><strong>往好里学</strong>（CLIP）：增加高分动作的概率。</li><li><strong>估得更准</strong>（VF）：让 Critic 对局面的打分更接近真实回报。</li><li><strong>别太死板</strong>（Entropy）：保留一些随机性，万一还有更好的路呢？</li></ul><h2 id="cartpole游戏">🎮 CartPole游戏</h2><div class="note note-info"><p><strong>Coming Soon！</strong></p></div></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/" class="print-no-link">#Research</a> <a href="/tags/Algorithm/" class="print-no-link">#Algorithm</a> <a href="/tags/PPO/" class="print-no-link">#PPO</a> <a href="/tags/OpenAI/" class="print-no-link">#OpenAI</a></div></div><div class="license-box my-3"><div class="license-title"><div>PPO——近端策略优化算法</div><div>https://choucisan.xyz/PPO.html</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>choucisan</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>December 15, 2025</div></div><div class="license-meta-item"><div>Licensed under</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/DPO.html" title="DPO——直接偏好优化算法"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">DPO——直接偏好优化算法</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/AlphaGomoku.html" title="AlphaGomoku——MCTS算法与五子棋"><span class="hidden-mobile">AlphaGomoku——MCTS算法与五子棋</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",function(){var i=Object.assign({appId:"H13sJOhFUDSFRB5l6uGK3P1R-gzGzoHsz",appKey:"9PsUJ7XI7U3Pg3ixfxWfPD7B",path:"window.location.pathname",placeholder:"Say something / 说点什么吧~",avatar:"retro",meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"en",highlight:!1,recordIP:!1,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!1},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)})})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>Table of Contents</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a></main><footer><div class="footer-inner"><div class="footer-content">© 2025 13 Lab. All Rights Reserved.</div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">Views: <span id="leancloud-site-pv"></span> </span><span id="leancloud-site-uv-container" style="display:none">Visitors: <span id="leancloud-site-uv"></span></span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t=t.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback(function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())})</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/custom-logic.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>