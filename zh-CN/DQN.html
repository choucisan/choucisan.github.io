<!DOCTYPE html><html lang="en,zh-CN"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="codeva-X99uCprQdB"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="choucisan"><meta name="keywords" content=""><meta name="description" content="引言： DQN（Deep Q-Network）是一种基于深度学习的强化学习算法，它通过构建一个神经网络来估计状态-动作值函数（Q函数），从而实现智能体的决策过程。DQN算法的核心思想是通过不断与环境交互，学习到最优的Q函数，从而找到最优的策略。本文将介绍DQN算法的基本原理、实现方法。             ✈️ DQN算法介绍 算法归类 从分类体系上看，DQN 完美继"><meta property="og:type" content="article"><meta property="og:title" content="DQN——Deep Q-Learning算法"><meta property="og:url" content="https://choucisan.xyz/zh-CN/DQN.html"><meta property="og:site_name" content="choucisan&#39;s blog"><meta property="og:description" content="引言： DQN（Deep Q-Network）是一种基于深度学习的强化学习算法，它通过构建一个神经网络来估计状态-动作值函数（Q函数），从而实现智能体的决策过程。DQN算法的核心思想是通过不断与环境交互，学习到最优的Q函数，从而找到最优的策略。本文将介绍DQN算法的基本原理、实现方法。             ✈️ DQN算法介绍 算法归类 从分类体系上看，DQN 完美继"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://images.unsplash.com/photo-1530880587016-3b71d746ab6c?q=80&w=2070&auto=format&fit=crop"><meta property="article:published_time" content="2025-12-10T09:58:52.000Z"><meta property="article:modified_time" content="2025-12-12T12:39:08.743Z"><meta property="article:author" content="choucisan"><meta property="article:tag" content="Research"><meta property="article:tag" content="Algorithm"><meta property="article:tag" content="DQN"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://images.unsplash.com/photo-1530880587016-3b71d746ab6c?q=80&w=2070&auto=format&fit=crop"><title>DQN——Deep Q-Learning算法 - choucisan&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link rel="stylesheet" href="/css/fullscreen-style.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"choucisan.xyz",root:"/",version:"1.9.8",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"VnqS06x2hxlj7MEjoUAix6L5-gzGzoHsz",app_key:"ck0fCo8cxJs0OdTEdWmSl9l1",server_url:"https://vnqs06x2.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2025-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>13log</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><span>HOME</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><span>ARCHIVES</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>TOPICS</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="/categories/Pub/" target="_self"><span>Publications</span> </a><a class="dropdown-item" href="/categories/OpenSource/" target="_self"><span>Open Source</span> </a><a class="dropdown-item" href="/categories/Notes/" target="_self"><span>Notes Share</span> </a><a class="dropdown-item" href="/categories/Tech/" target="_self"><span>Tech Share</span> </a><a class="dropdown-item" href="/categories/Life/" target="_self"><span>Life Share</span></a></div></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><span>ABOUT</span></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">DQN——Deep Q-Learning算法</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-12-10 17:58" pubdate>2025年12月10日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 31 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">DQN——Deep Q-Learning算法</h1><div class="markdown-body"><div class="note note-info"><p><strong>引言：</strong> DQN（Deep Q-Network）是一种基于深度学习的强化学习算法，它通过构建一个神经网络来估计状态-动作值函数（Q函数），从而实现智能体的决策过程。DQN算法的核心思想是通过不断与环境交互，学习到最优的Q函数，从而找到最优的策略。本文将介绍DQN算法的基本原理、实现方法。</p></div><h2 id="dqn算法介绍">✈️ DQN算法介绍</h2><h3 id="算法归类">算法归类</h3><p>从分类体系上看，DQN 完美继承了 Q-Learning 的基因，同时引入了深度学习的特性：</p><ul><li><strong>基于价值 (Value-based)</strong>：DQN 不直接输出动作的概率，而是输出每个动作的价值（Q值），通过 <span class="math inline">arg max <em>Q</em></span> 来选择动作。</li><li><strong>无模型 (Model-free)</strong>：智能体不需要理解环境的物理规律（如重力、摩擦力），直接通过试错来学习。</li><li><strong>离轨策略 (Off-policy)</strong>：DQN 使用经验回放池（Experience Replay），这意味着它学习的数据来自于“过去的自己”甚至“随机策略”，而不是当前的策略。</li><li><strong>深度强化学习 (Deep RL)</strong>：这是它与 Q-Learning 最大的区别，使用神经网络（Function Approximator）来代替表格。</li></ul><h3 id="适用范围与局限">适用范围与局限</h3><ul><li><strong>适用范围</strong>：<ul><li><strong>高维状态空间</strong>：这是 DQN 的杀手锏。它可以直接处理图像（像素矩阵）、复杂的传感器数据，解决了表格型方法无法处理“无限状态”的问题。</li><li><strong>离散动作空间</strong>：适用于像《王者荣耀》放技能、Atari 游戏（上下左右）这类动作有限的任务。</li></ul></li><li><strong>局限性</strong>：<ul><li><strong>难以处理连续动作</strong>：对于机器人关节控制（角度是连续小数）这类任务，DQN 需要将动作离散化，但这会导致动作维度爆炸。通常需要借助 Actor-Critic 类算法（如 DDPG）来解决。</li><li><strong>训练不稳定</strong>：相比于简单的查表，神经网络的收敛极其敏感，对超参数（学习率、Replay Buffer 大小等）要求很高。</li></ul></li></ul><h3 id="基本思想">基本思想</h3><p>在 Q-Learning 中，我们有一个真实的表格 <span class="math inline"><em>Q</em><sub><em>t</em><em>a</em><em>b</em><em>l</em><em>e</em></sub>(<em>s</em>, <em>a</em>)</span>。 在 DQN 中，我们抛弃表格，用一个神经网络 <span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>; <em>θ</em>)</span> 来<strong>拟合</strong>这个价值函数。</p><ul><li><strong>输入</strong>：状态 <span class="math inline"><em>s</em></span>（例如一张 <span class="math inline">84 × 84</span> 的游戏截图）。</li><li><strong>经过</strong>：卷积层 (CNN) 提取特征 <span class="math inline">→</span> 全连接层 (FC) 进行推理。</li><li><strong>输出</strong>：所有可能动作的 Q 值向量。例如 <code>[Q(s, 左), Q(s, 右), Q(s, 跳)]</code>。</li></ul><p>我们的目标是训练参数 <span class="math inline"><em>θ</em></span>，使得网络预测的 Q 值逼近真实的期望回报。</p><hr><h2 id="dqn算法流程">🚀 DQN算法流程</h2><p>DQN 之所以能成功，是因为它引入了两个关键机制来解决神经网络训练不稳定的问题：<strong>经验回放</strong>和<strong>目标网络</strong>。</p><figure><img src="/images/rl/dpn.webp" srcset="/img/loading.gif" lazyload alt="DQN算法流程"><figcaption aria-hidden="true">DQN算法流程</figcaption></figure><h3 id="经验回放-experience-replay">经验回放 (Experience Replay)</h3><p><strong>问题</strong>：在玩游戏时，时间是连续的。这一秒的状态 <span class="math inline"><em>s</em><sub><em>t</em></sub></span> 和下一秒的状态 <span class="math inline"><em>s</em><sub><em>t</em> + 1</sub></span> 非常相似。如果直接把这些高度相关的数据喂给神经网络，网络会“钻牛角尖”（过拟合），导致训练发散。</p><p><strong>解决方案</strong>：建立一个<strong>记忆库 (Replay Buffer)</strong>。</p><ul><li><strong>存储</strong>：智能体与环境交互，把每一步产生的数据 <span class="math inline">(<em>s</em>, <em>a</em>, <em>r</em>, <em>s</em><sup>′</sup>)</span> 存入一个容量有限（例如 100万）的队列中。</li><li><strong>采样</strong>：在训练时，不按顺序学习，而是从记忆库中<strong>随机抽取 (Random Batch)</strong> 一批数据（例如 32 条）来进行梯度下降。</li></ul><p><strong>作用</strong>：</p><ul><li><strong>打乱相关性</strong>：随机采样消除了数据之间的时间相关性，满足了神经网络训练所需的独立同分布 (I.I.D) 假设。</li><li><strong>数据复用</strong>：一条经验可以被多次抽取和学习，提高了数据利用率。</li></ul><h3 id="目标网络-target-network">目标网络 (Target Network)</h3><p><strong>问题</strong>：在 Q-Learning 的更新公式中，目标值 <span class="math inline"><em>T</em><em>a</em><em>r</em><em>g</em><em>e</em><em>t</em> = <em>r</em> + <em>γ</em>max <em>Q</em>(<em>s</em><sup>′</sup>)</span>。 在 DQN 中，如果只用一个网络，那么 <span class="math inline"><em>Q</em>(<em>s</em>)</span>（当前估计）和 <span class="math inline"><em>Q</em>(<em>s</em><sup>′</sup>)</span>（目标估计）都由同一个网络参数 <span class="math inline"><em>θ</em></span> 决定。 这就像<strong>“左脚踩右脚上天”</strong>——每次更新参数 <span class="math inline"><em>θ</em></span>，不仅预测值变了，目标值也变了。目标忽高忽低，导致网络很难收敛。</p><p><strong>解决方案</strong>：使用两个结构相同但参数独立的网络。</p><ul><li><strong>当前网络 (Eval Net, 参数 <span class="math inline"><em>θ</em></span>)</strong>：负责选择动作，并利用梯度下降实时更新参数。这是我们需要训练的“主网络”。</li><li><strong>目标网络 (Target Net, 参数 <span class="math inline"><em>θ</em><sup>−</sup></span>)</strong>：专门负责计算目标 Q 值。它的参数<strong>不会实时更新</strong>，而是每隔一定步数（例如 C=1000 步）才从当前网络复制一次。</li></ul><p><strong>作用</strong>： 通过“固定”目标网络一段时间，相当于给训练提供了一个<strong>暂时静止的靶子</strong>，极大地提高了训练的稳定性。</p><h3 id="目标网络梯度更新">目标网络梯度更新</h3><p>DQN 的训练本质上是一个<strong>回归问题 (Regression)</strong>。我们要最小化“当前网络的预测值”和“目标网络计算的真实值”之间的差距。</p><p><strong>计算目标值 (TD Target)</strong> 对于采样出来的每一条数据 <span class="math inline">(<em>s</em>, <em>a</em>, <em>r</em>, <em>s</em><sup>′</sup>)</span>，利用<strong>目标网络</strong>计算标签：</p><p><span class="math display"><em>y</em> = <em>r</em> + <em>γ</em>max<sub><em>a</em><sup>′</sup></sub><em>Q</em>(<em>s</em><sup>′</sup>, <em>a</em><sup>′</sup>; <em>θ</em><sup>−</sup>)</span></p><ul><li>注意这里使用的是 <span class="math inline"><em>θ</em><sup>−</sup></span>（目标网络的旧参数）。</li></ul><p><strong>计算预测值 (Current Prediction)</strong> 利用<strong>当前网络</strong>计算当前状态动作的价值：</p><p><span class="math display"><em>ŷ</em> = <em>Q</em>(<em>s</em>, <em>a</em>; <em>θ</em>)</span></p><ul><li>注意这里使用的是 <span class="math inline"><em>θ</em></span>（当前网络的最新参数）。</li></ul><p><strong>损失函数 (Loss Function)</strong> 使用均方误差 (MSE) 来衡量两者的差距：</p><p><span class="math display"><em>L</em>(<em>θ</em>) = 𝔼[(<em>y</em> − <em>ŷ</em>)<sup>2</sup>] = 𝔼[(<em>r</em> + <em>γ</em>max<sub><em>a</em><sup>′</sup></sub><em>Q</em>(<em>s</em><sup>′</sup>, <em>a</em><sup>′</sup>; <em>θ</em><sup>−</sup>) − <em>Q</em>(<em>s</em>, <em>a</em>; <em>θ</em>))<sup>2</sup>]</span></p><p><strong>梯度反向传播</strong> 对损失函数 <span class="math inline"><em>L</em>(<em>θ</em>)</span> 关于 <span class="math inline"><em>θ</em></span> 求导，并使用 SGD 或 Adam 优化器更新当前网络的参数：</p><p><span class="math display"><em>θ</em> ← <em>θ</em> − <em>α</em>∇<sub><em>θ</em></sub><em>L</em>(<em>θ</em>)</span></p><p>通过不断的迭代，当前网络 <span class="math inline"><em>Q</em></span> 会越来越精准，最终学会最优策略。</p><h2 id="think-different">🤔 Think Different</h2><p>我们经常看到一个有趣的现象：<strong>在不同领域、不同时间被发明的算法，其核心数学原理竟然惊人地一致。</strong></p><p>DQN 中生硬的<strong>硬更新（Hard Update）</strong>，在后来的 <strong>DDPG</strong> 算法中演化为了丝滑的<strong>软更新（Soft Update）</strong>，而这一思想后来竟成为了无监督学习（如 MoCo）的基石。这证明了<strong>通过平滑目标来稳定训练</strong>是一个通用的、解决共性问题的方法论。</p><h3 id="目标函数的更新方法">目标函数的更新方法</h3><p><strong>DQN (2015)</strong> 为了稳定训练，使用了 <strong>Hard Update</strong>：</p><ul><li><strong>机制</strong>：目标网络 <span class="math inline"><em>θ</em><sup>−</sup></span> 平时完全不动，每隔 <span class="math inline"><em>C</em></span> 步（比如 1000 步），直接把当前网络 <span class="math inline"><em>θ</em></span> 的参数<strong>完全复制</strong>过去。</li><li><strong>缺点</strong>：目标值 <span class="math inline"><em>y</em></span> 会呈现“阶梯状”跳变。在复制参数的那一瞬间，目标值突然大幅变化，可能导致 Loss 瞬间飙升，训练过程产生剧烈震荡。</li></ul><p><strong>DDPG (2016)</strong> 为了处理连续动作空间，对稳定性要求更高，因此引入了 <strong>Soft Update</strong>：</p><ul><li><strong>机制</strong>：目标网络<strong>每一步</strong>都更新，但每次只更新<strong>一点点</strong>。</li><li><strong>公式</strong>： <span class="math display"><em>θ</em><sup>−</sup> ← <em>τ</em><em>θ</em> + (1 − <em>τ</em>)<em>θ</em><sup>−</sup></span> 其中 <span class="math inline"><em>τ</em></span> 是一个极小的系数（例如 <span class="math inline"><em>τ</em> = 0.001</span>）。</li><li><strong>效果</strong>：这意味着目标网络 <span class="math inline"><em>θ</em><sup>−</sup></span> 是当前网络 <span class="math inline"><em>θ</em></span> 的<strong>指数移动平均 (Exponential Moving Average, EMA)</strong>。目标值的变化不再是剧烈的跳变，而是像“斜坡”一样平滑过渡。</li></ul><h3 id="无监督学习中的软更新">无监督学习中的“软更新”</h3><p>当我们把 DDPG 的软更新公式，和几年后计算机视觉领域 <strong>MoCo (Momentum Contrast)</strong> 的更新公式放在一起时，会发现它们完全是一回事：</p><table><thead><tr><th style="text-align:left">算法</th><th style="text-align:left">领域</th><th style="text-align:left">术语</th><th style="text-align:left">公式 (<span class="math inline"><em>m</em> = 1 − <em>τ</em></span>)</th><th style="text-align:left">目的</th></tr></thead><tbody><tr><td style="text-align:left"><strong>DDPG</strong></td><td style="text-align:left">强化学习 (RL)</td><td style="text-align:left">Soft Update</td><td style="text-align:left"><span class="math inline"><em>θ</em><sup>−</sup> ← <em>τ</em><em>θ</em> + (1 − <em>τ</em>)<em>θ</em><sup>−</sup></span></td><td style="text-align:left">稳定 TD Target，防止策略震荡</td></tr><tr><td style="text-align:left"><strong>MoCo</strong></td><td style="text-align:left">自监督学习 (SSL)</td><td style="text-align:left">Momentum Update</td><td style="text-align:left"><span class="math inline"><em>θ</em><sub><em>k</em></sub> ← <em>m</em><em>θ</em><sub><em>k</em></sub> + (1 − <em>m</em>)<em>θ</em><sub><em>q</em></sub></span></td><td style="text-align:left">维持队列特征一致性，防止特征剧变</td></tr></tbody></table><ul><li><strong>DDPG 的 <span class="math inline"><em>τ</em> ≈ 0.001</span></strong> 对应了 <strong>MoCo 的 <span class="math inline"><em>m</em> ≈ 0.999</span></strong>。</li><li>它们都在做同一件事：<strong>制造一个“沉稳的老师”</strong>。<ul><li>即使“学生”（当前网络/Query Encoder）因为学习率高、Batch 噪声大而上蹿下跳；</li><li>“老师”（目标网络/Key Encoder）依然保持淡定，只吸收学生长期的平均趋势，过滤掉高频的噪声。</li></ul></li></ul><h3 id="为什么好的方法是通用的">为什么好的方法是通用的？</h3><p>为什么这种<strong>参数的凸组合（Convex Combination）</strong>能解决共性问题？本质上有三个原因：</p><ul><li><p><strong>低通滤波 (Low-Pass Filtering)</strong>： 深度学习的训练过程充满了随机噪声（SGD 的随机性、环境采样的随机性）。软更新本质上是一个<strong>低通滤波器</strong>。它过滤掉了参数更新中的高频震荡（Noise），只保留了参数演化的低频趋势（Trend）。无论是 RL 还是 SSL，只要存在非平稳性，这种滤波都是必须的。</p></li><li><p><strong>滞后带来稳定 (Lag implies Stability)</strong>： 控制理论告诉我们，在一个闭环反馈系统（Self-Loop）中，如果反馈过于灵敏，系统容易发散（啸叫）。通过软更新引入<strong>滞后 (Lag)</strong>，相当于增加了系统的阻尼，虽然减慢了响应速度，但极大地提高了系统的鲁棒性。</p></li><li><p><strong>平滑流形 (Smooth Manifold)</strong>： 在 DDPG 面对的连续控制和 MoCo 面对的高维特征空间中，参数的微小跳变可能导致输出结果在流形上发生巨大位移。软更新保证了目标函数在优化表面上的轨迹是<strong>连续且平滑</strong>的，这对于基于梯度的优化器来说是最友好的环境。</p></li></ul><p>从 DQN 的<strong>Hard Update</strong>，到 DDPG 的<strong>Soft Update</strong>，再到 MoCo 的<strong>Momentum Update</strong>，看到了：</p><blockquote><p>通过牺牲即时的更新速度（利用旧参数），换取了训练目标的稳定性（一致性），这种<strong>以退为进</strong>的智慧，是解决所有<strong>自我博弈（Self-Supervised/Bootstrapping）</strong>类问题的通用钥匙。</p></blockquote><h2 id="cartpole游戏">🎮 CartPole游戏</h2><div class="note note-info"><p><strong>Coming Soon！</strong></p></div></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Notes/" class="category-chain-item">Notes</a> <span>></span> <a href="/categories/Notes/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/" class="print-no-link">#Research</a> <a href="/tags/Algorithm/" class="print-no-link">#Algorithm</a> <a href="/tags/DQN/" class="print-no-link">#DQN</a></div></div><div class="license-box my-3"><div class="license-title"><div>DQN——Deep Q-Learning算法</div><div>https://choucisan.xyz/DQN.html</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>choucisan</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年12月10日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/AlphaGomoku.html" title="AlphaGomoku——MCTS算法与五子棋"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">AlphaGomoku——MCTS算法与五子棋</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/Q-Learning.html" title="Q-Learning——一种基于值迭代的强化学习算法"><span class="hidden-mobile">Q-Learning——一种基于值迭代的强化学习算法</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",function(){var i=Object.assign({appId:"H13sJOhFUDSFRB5l6uGK3P1R-gzGzoHsz",appKey:"9PsUJ7XI7U3Pg3ixfxWfPD7B",path:"window.location.pathname",placeholder:"Say something / 说点什么吧~",avatar:"retro",meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"en",highlight:!1,recordIP:!1,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!1},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)})})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a></main><footer><div class="footer-inner"><div class="footer-content">© 2025 13 Lab. All Rights Reserved.</div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t=t.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback(function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())})</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/custom-logic.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>