<!DOCTYPE html><html lang="en,zh-CN"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="codeva-rqRClcMtgO"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="choucisan"><meta name="keywords" content=""><meta name="description" content="引言： 蒙特卡洛搜索树（Monte Carlo Tree Search, MCTS）是一种用于决策的搜索算法，广泛应用于游戏、人工智能等领域。本文将介绍 MCTS 的基本原理，并通过一个五子棋的项目来展示如何使用 MCTS 算法进行游戏决策。             📖 蒙特卡洛搜索树（MCTS） MCTS是什么？ MCTS是一种用于决策过程的启发式搜索算法，最早在20"><meta property="og:type" content="article"><meta property="og:title" content="AlphaGomoku——MCTS算法与五子棋"><meta property="og:url" content="https://choucisan.xyz/zh-CN/AlphaGomoku.html"><meta property="og:site_name" content="choucisan&#39;s blog"><meta property="og:description" content="引言： 蒙特卡洛搜索树（Monte Carlo Tree Search, MCTS）是一种用于决策的搜索算法，广泛应用于游戏、人工智能等领域。本文将介绍 MCTS 的基本原理，并通过一个五子棋的项目来展示如何使用 MCTS 算法进行游戏决策。             📖 蒙特卡洛搜索树（MCTS） MCTS是什么？ MCTS是一种用于决策过程的启发式搜索算法，最早在20"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://choucisan.xyz/images/gomoku/alphago.png"><meta property="article:published_time" content="2025-12-05T13:57:18.000Z"><meta property="article:modified_time" content="2025-12-06T09:30:07.841Z"><meta property="article:author" content="choucisan"><meta property="article:tag" content="Research"><meta property="article:tag" content="MCTS"><meta property="article:tag" content="AlphaGo"><meta property="article:tag" content="Project"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://choucisan.xyz/images/gomoku/alphago.png"><title>AlphaGomoku——MCTS算法与五子棋 - choucisan&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link rel="stylesheet" href="/css/fullscreen-style.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"choucisan.xyz",root:"/",version:"1.9.8",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"VnqS06x2hxlj7MEjoUAix6L5-gzGzoHsz",app_key:"ck0fCo8cxJs0OdTEdWmSl9l1",server_url:"https://vnqs06x2.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2025-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>13log</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><span>HOME</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><span>ARCHIVES</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>TOPICS</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="/categories/Pub/" target="_self"><span>Publications</span> </a><a class="dropdown-item" href="/categories/OpenSource/" target="_self"><span>Open Source</span> </a><a class="dropdown-item" href="/categories/Paper/" target="_self"><span>Paper Reading</span> </a><a class="dropdown-item" href="/categories/Tech/" target="_self"><span>Tech Share</span> </a><a class="dropdown-item" href="/categories/Life/" target="_self"><span>Life Share</span></a></div></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><span>ABOUT</span></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">AlphaGomoku——MCTS算法与五子棋</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-12-05 21:57" pubdate>2025年12月5日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 31 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">AlphaGomoku——MCTS算法与五子棋</h1><div class="markdown-body"><div class="note note-info"><p><strong>引言：</strong> 蒙特卡洛搜索树（Monte Carlo Tree Search, MCTS）是一种用于决策的搜索算法，广泛应用于游戏、人工智能等领域。本文将介绍 MCTS 的基本原理，并通过一个五子棋的项目来展示如何使用 MCTS 算法进行游戏决策。</p></div><h2 id="蒙特卡洛搜索树mcts">📖 蒙特卡洛搜索树（MCTS）</h2><h3 id="mcts是什么">MCTS是什么？</h3><p>MCTS是一种用于决策过程的启发式搜索算法，最早在2007年由Guillaume Chaslot在<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AIIDE/article/view/18700">《Monte Carlo Tree Search: A New Framework for Game Backgammon》</a>一文中提出。它通过模拟游戏过程来评估不同决策的优劣。MCTS的主要思想是通过反复模拟游戏过程，来估计每个决策的期望收益，从而选择最优决策。MCTS 算法通常由以下四个步骤组成：</p><ul><li><p><strong>选择（Selection）</strong>：从当前树的根节点出发，沿着子节点逐层选择一个节点，直到到达一个未完全展开的节点或达到最大模拟深度为止。在这个过程中，选择最有可能导致最佳结果的节点，通常使用的是 UCB（上置信区间）策略。</p></li><li><p><strong>扩展（Expansion）</strong>： 如果所选节点不是终端节点，从该节点中随机选择一个未被访问过的子节点，添加到搜索树中。</p></li><li><p><strong>模拟（Simulation）</strong>：从新扩展的节点开始，进行随机模拟（也称为“Playout”，通常使用随机策略），直到达到游戏的终局状态。根据终局状态的结果（如胜负或得分），评估该模拟。</p></li><li><p><strong>反向传播（Backpropagation）</strong>： 模拟结束后，将模拟结果（如胜负）回溯到树的每个节点。每个节点更新自己的统计数据，记录该节点的胜率或平均奖励。</p></li></ul><figure><img src="/images/gomoku/mcts.png" srcset="/img/loading.gif" lazyload alt="MCTS 算法图示"><figcaption aria-hidden="true">MCTS 算法图示</figcaption></figure><h2 id="如何理解mcts">🎯 如何理解MCTS？</h2><p>我们以下五子棋为例，讲解MCTS算法的原理。</p><h3 id="选择selection">选择（Selection）</h3><p>比如，当轮到我们下棋时，我们会在脑海中进行推演：我们下在A点，对手可能会下在B点，然后我们下在C点，对手可能会下在D点，以此类推。这个过程就是MCTS算法中的选择（Selection）过程。我们通过模拟不同的下棋路径，来选择一个最有利的下棋点。</p><p>这个过程的每一步选择都可以建模成一个<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/多臂赌博机">多臂老虎机问题</a>，即每次下棋都相当于选择一个老虎机臂，然后通过模拟来估计这个臂的收益。我们希望选择一个收益最大的臂，即选择一个最有利的下棋点。</p><p>那么，如何选择一个最有利的下棋点呢？我们可以使用上置信界（Upper Confidence Bound，UCB）算法，即选择一个收益最大的臂，同时考虑臂的探索程度。具体来说，UCB策略的计算公式为：</p><p><span class="math display">$$ UCB = \frac{w}{n} + c \sqrt{\frac{\ln N}{n}} $$</span></p><p>其中，<span class="math inline"><em>w</em></span>是臂的收益，<span class="math inline"><em>n</em></span>是臂被选择的次数，<span class="math inline"><em>N</em></span>是所有臂被选择的次数，<span class="math inline"><em>c</em></span>是一个常数，用于控制探索和利用的平衡。通过这个公式，我们可以选择一个收益最大的臂，同时考虑臂的探索程度。</p><div class="note note-primary"><p><strong>探索（Exploration</strong>）和<strong>利用（Exploitation）</strong>是强化学习中的一个重要概念。</p><ul><li><strong>探索</strong>：基于已有知识选择奖励最大的动作。</li><li><strong>利用</strong>：选择当前已知的最优动作，以获得最大的奖励。</li></ul><p>有效的策略通常在探索与利用之间找到平衡。过度利用可能导致智能体停留在次优解，而过度探索则可能浪费资源。在MCTS算法中，通过UCB策略，可以在探索和利用之间找到平衡，从而选择一个最有利的下棋点。</p></div><h3 id="扩展expansion">扩展（Expansion）</h3><p>比如，我们在脑海中推演到了一个新的局面：我们下在A点，对手应在了B点，此时棋盘出现了一个我们从未深入思考过的布局。我们需要决定下一步该往哪里探索，这个将新的可能性加入我们“推演地图”的过程，就是扩展（Expansion）。</p><p>在算法中，当“选择”过程走到了一个叶子节点（即还没有被完全推演过的节点），只要游戏未结束，我们就会创建一个或多个新的子节点（比如尝试下在C点或D点），并将这些新节点添加到我们的搜索树中。这相当于我们在脑海的棋谱中，新开辟了一条未知的路径，准备一探究竟。</p><h3 id="模拟simulation">模拟（Simulation）</h3><p>既然我们要一探究竟，就需要知道这条新路径到底好不好。但是我们不可能穷尽之后的所有变化。于是，我们采用一种“快速推演”的方法：从这个新节点开始，在脑海中快速地推演（通常是随机地）棋局走向，直到分出胜负或平局。这个过程就是模拟（Simulation）。</p><p>这就像我们在脑海中按下了“快进键”，不再精细计算每一步，而是大致预估一下：“如果局面发展成这样，最后谁赢面大？”。虽然这种随机模拟单次看起来很粗糙，但只要模拟次数足够多，它就能有效地反映出该局面的优劣。</p><h3 id="反向传播backpropagation">反向传播（Backpropagation）</h3><p>当模拟结束后，我们得到了一个结果（比如我们输了），我们就会总结这个失败的经验，告诉自己下次避免走这个路径。这个过程就是反向传播（Backpropagation）。</p><p>在算法中，我们会沿着刚才选择的路径从叶子节点回到根节点，更新路径上每个节点的统计数据：</p><ul><li><span class="math inline"><em>n</em></span>（访问次数）：该节点参与推演的次数 <span class="math inline">+1</span>。</li><li><span class="math inline"><em>w</em></span>（累计收益）：如果模拟结果是我们赢了，则该节点的胜利分数增加。</li></ul><p>通过不断地循环这四个步骤，我们的搜索树会越来越庞大，统计数据也越来越准确，最终指引我们下出最强的一步棋。</p><h2 id="mcts的应用">📰 MCTS的应用</h2><h3 id="mcts与alphago">MCTS与AlphaGo</h3><p>AlphaGo 是由 Google DeepMind 开发的人工智能围棋程序，它的核心正是采用了 <strong>MCTS（蒙特卡洛树搜索）与深度神经网络（策略网络和价值网络）相结合</strong> 的架构。</p><p>2015年10月，AlphaGo 与三届欧洲围棋冠军樊麾进行了首场秘密对决。AlphaGo 以 <strong>5:0</strong> 的比分完胜，这是人工智能系统首次在无让子的情况下击败职业围棋选手。</p><p>2016年3月，AlphaGo 挑战曾获得18次世界冠军的围棋传奇人物李世石。这场比赛吸引了全球超过2亿人观看。最终，AlphaGo 以 <strong>4:1</strong> 的比分取得胜利。 <img src="/images/gomoku/game.png" srcset="/img/loading.gif" lazyload alt="AlphaGo 与 李世石的围棋对决"></p><h3 id="mcts与llm">MCTS与LLM</h3><p>随着 GPT-4 等大语言模型的爆发，NLP 领域取得了惊人的进展。然而，LLM 在处理数学证明、复杂逻辑推理时，仍面临“一本正经胡说八道”（幻觉）的问题。为了解决这一问题，研究界开始尝试将 <strong>System 2（慢思考/逻辑推理）</strong> 引入 LLM，而 MCTS 正是实现这一目标的最佳框架之一。</p><h4 id="mctsr-提升数学推理能力">MCTSr —— 提升数学推理能力</h4><ul><li><strong>📄 论文</strong>：《Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B》</li><li><strong>🏛️ 机构</strong>：复旦大学、上海人工智能实验室等</li></ul><p>在数学奥林匹克等高精度任务中，LLM 往往因为一步推理错误而导致全盘皆输。为了解决这个问题，研究团队提出了 <strong>MCT Self-Refine (MCTSr)</strong> 算法。</p><ul><li><strong>核心痛点</strong>：LLM 的生成是概率性的，容易产生幻觉，缺乏自我纠错机制。</li><li><strong>解决方案</strong>：<ul><li>将解题过程建模为 MCTS 的搜索树。</li><li><strong>系统探索</strong>：利用 MCTS 探索不同的解题路径。</li><li><strong>自我修正</strong>：结合 LLM 的自我反思能力（Self-Refine），对搜索过程中的节点进行评估和细化。</li></ul></li><li><strong>成果</strong>：该方法成功利用较小的模型（LLaMa-3 8B）实现了媲美 GPT-4 级别的数学解题能力。</li></ul><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.07394">🔗 阅读论文 (arXiv:2406.07394)</a></p><h4 id="marco-o1-扩展开放式推理空间">Marco-o1 —— 扩展开放式推理空间</h4><ul><li><strong>📄 论文</strong>：《Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions》</li><li><strong>🏛️ 机构</strong>：阿里研究团队</li></ul><p>受 OpenAI o1 模型卓越推理能力的启发，阿里团队开源了 <strong>Marco-o1</strong> 推理模型，旨在解决更复杂的现实世界挑战。</p><ul><li><strong>核心目标</strong>：探索 o1 类的推理模型能否推广到缺乏明确标准答案、且奖励难以量化的<strong>开放式问题</strong>领域。</li><li><strong>解决方案 (Marco-o1-MCTS)</strong>：<ul><li><strong>扩展解空间</strong>：将 LLM 与 MCTS 结合，允许模型尝试多种推理路径。</li><li><strong>置信度引导</strong>：利用模型输出的置信度作为信号，指导 MCTS 的搜索方向，从而在复杂的解空间中找到更优解。</li></ul></li><li><strong>意义</strong>：通过 MCTS 扩展了模型的思考维度，使其在 AIME 和 CodeForces 等平台上展现出超越传统模型的推理能力。</li></ul><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.14405">🔗 阅读论文 (arXiv:2411.14405)</a></p><h2 id="mtcs与五子棋游戏">🚀 MTCS与五子棋游戏</h2><p>这里我们以五子棋游戏为例，展示如何使用 MCTS 算法进行游戏决策。</p><div class="note note-info"><p><strong>Coming Soon！</strong></p></div><p></p><div class="note note-success"><ul><li><strong>如何获取完整代码？</strong></li><li><strong>Github链接：<a target="_blank" rel="noopener" href="https://github.com/choucisan/alpahagomoku">https://github.com/choucisan/alpahagomoku</a></strong></li><li><strong>关注博客，以后会不定期更新！</strong></li></ul></div><h4 id="参考资料与推荐阅读">参考资料与推荐阅读</h4><p>本文参考了以下资料，这些论文和博客非常有参考价值，推荐大家阅读：</p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238">General Game-Playing With Monte Carlo Tree Search</a> <a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="http://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/">Introduction to Monte Carlo Tree Search</a> <a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://deepmind.google/research/alphago/">DeepMind: AlphaGo Research</a> <a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.07394">Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</a> <a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.14405">Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</a> <a href="#fnref:5" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Tech/" class="category-chain-item">Tech</a> <span>></span> <a href="/categories/Tech/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/" class="print-no-link">#Research</a> <a href="/tags/MCTS/" class="print-no-link">#MCTS</a> <a href="/tags/AlphaGo/" class="print-no-link">#AlphaGo</a> <a href="/tags/Project/" class="print-no-link">#Project</a></div></div><div class="license-box my-3"><div class="license-title"><div>AlphaGomoku——MCTS算法与五子棋</div><div>https://choucisan.xyz/AlphaGomoku.html</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>choucisan</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年12月5日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/AlphaGo.html" title="AlphaGo——Mastering the game of Go with deep neural networks and tree search"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">AlphaGo——Mastering the game of Go with deep neural networks and tree search</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/BuildBlog.html" title="BlogShare——如何做好一个博客"><span class="hidden-mobile">BlogShare——如何做好一个博客</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",function(){var i=Object.assign({appId:"H13sJOhFUDSFRB5l6uGK3P1R-gzGzoHsz",appKey:"9PsUJ7XI7U3Pg3ixfxWfPD7B",path:"window.location.pathname",placeholder:"Say something / 说点什么吧~",avatar:"retro",meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"en",highlight:!1,recordIP:!1,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!1},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)})})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a></main><footer><div class="footer-inner"><div class="footer-content">© 2025 13 Lab. All Rights Reserved.</div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t=t.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback(function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())})</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/custom-logic.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>