<!DOCTYPE html><html lang="en,zh-CN"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="codeva-X99uCprQdB"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="choucisan"><meta name="keywords" content=""><meta name="description" content="引言： 最近在研究强化学习，所以打算写一系列关于强化学习的文章(实时更新)，包括Q-learning、DQN、MCTS、PPO、DPO、RLHF、GPRO等，这里是一些基础知识和后续文章的目录，可以在这里直接跳转你感兴趣的内容。             📖 RL系列文章目录 以后有关于强化学习的算法与文章都会在这里更新，目录如下： 1.Q-Learning——一种基于值"><meta property="og:type" content="article"><meta property="og:title" content="Reinforcement Learning——RL系列目录"><meta property="og:url" content="https://choucisan.xyz/zh-CN/RL.html"><meta property="og:site_name" content="choucisan&#39;s blog"><meta property="og:description" content="引言： 最近在研究强化学习，所以打算写一系列关于强化学习的文章(实时更新)，包括Q-learning、DQN、MCTS、PPO、DPO、RLHF、GPRO等，这里是一些基础知识和后续文章的目录，可以在这里直接跳转你感兴趣的内容。             📖 RL系列文章目录 以后有关于强化学习的算法与文章都会在这里更新，目录如下： 1.Q-Learning——一种基于值"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://images.unsplash.com/photo-1721815885269-b6ba1004f0a5?q=80&w=2070&auto=format&fit=crop"><meta property="article:published_time" content="2025-12-08T09:58:52.000Z"><meta property="article:modified_time" content="2025-12-12T12:27:40.321Z"><meta property="article:author" content="choucisan"><meta property="article:tag" content="Research"><meta property="article:tag" content="Algorithm"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://images.unsplash.com/photo-1721815885269-b6ba1004f0a5?q=80&w=2070&auto=format&fit=crop"><title>Reinforcement Learning——RL系列目录 - choucisan&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link rel="stylesheet" href="/css/fullscreen-style.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"choucisan.xyz",root:"/",version:"1.9.8",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"VnqS06x2hxlj7MEjoUAix6L5-gzGzoHsz",app_key:"ck0fCo8cxJs0OdTEdWmSl9l1",server_url:"https://vnqs06x2.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2025-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>13log</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><span>HOME</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><span>ARCHIVES</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>TOPICS</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="/categories/Pub/" target="_self"><span>Publications</span> </a><a class="dropdown-item" href="/categories/OpenSource/" target="_self"><span>Open Source</span> </a><a class="dropdown-item" href="/categories/Paper/" target="_self"><span>Paper Reading</span> </a><a class="dropdown-item" href="/categories/Tech/" target="_self"><span>Tech Share</span> </a><a class="dropdown-item" href="/categories/Life/" target="_self"><span>Life Share</span></a></div></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><span>ABOUT</span></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">Reinforcement Learning——RL系列目录</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-12-08 17:58" pubdate>2025年12月8日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.4k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 37 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">Reinforcement Learning——RL系列目录</h1><div class="markdown-body"><div class="note note-info"><p><strong>引言：</strong> 最近在研究强化学习，所以打算写一系列关于强化学习的文章(实时更新)，包括Q-learning、DQN、MCTS、PPO、DPO、RLHF、GPRO等，这里是一些基础知识和后续文章的目录，可以在这里直接跳转你感兴趣的内容。</p></div><h2 id="rl系列文章目录">📖 RL系列文章目录</h2><p>以后有关于强化学习的算法与文章都会在这里更新，目录如下：</p><h3 id="q-learning一种基于值迭代的强化学习算法"><strong>1.<a href="https://choucisan.xyz/Q-Learning.html">Q-Learning——一种基于值迭代的强化学习算法</a></strong></h3><h3 id="dqndeep-q-learning算法"><strong>2.<a href="https://choucisan.xyz/DQN.html">DQN——Deep Q-Learning算法</a></strong></h3><h3 id="alphagomokumcts算法与五子棋"><strong>3.<a href="https://choucisan.xyz/AlphaGomoku.html">AlphaGomoku——MCTS算法与五子棋</a></strong></h3><h3 id="ppo近端策略优化算法"><strong>4.<a href="https://choucisan.xyz/PPO.html">PPO——近端策略优化算法</a></strong></h3><h3 id="dpo直接偏好优化算法"><strong>5.<a href="https://choucisan.xyz/DPO.html">DPO——直接偏好优化算法</a></strong></h3><h3 id="gpro组相对策略优化算法"><strong>6.<a href="https://choucisan.xyz/GPRO.html">GPRO——组相对策略优化算法</a></strong></h3><h2 id="强化学习的基本知识">📕 强化学习的基本知识</h2><h3 id="强化学习的分类">强化学习的分类</h3><p>根据不同的分类方式，可以将强化学习分为不同的类别，下面是几种常见的分类方式：</p><h4 id="是否显式地学习一个策略进行分类">是否显式地学习一个策略进行分类</h4><p>这种分类方式关注的是智能体（Agent）最终是直接学习“该怎么做”，还是通过学习“哪个动作更好”来间接决定。</p><ul><li><strong>基于价值（Value-based）</strong><ul><li><strong>原理</strong>：不直接学习策略，而是学习一个<strong>价值函数</strong>（如 Q 函数）。它主要预测在某个状态下采取某个动作的未来预期回报。决策时，通常选择价值最大的动作（贪婪策略）。</li><li><strong>特点</strong>：通常是确定性的。</li><li><strong>代表算法</strong>：Q-Learning, DQN, Sarsa。</li></ul></li><li><strong>基于策略（Policy-based）</strong><ul><li><strong>原理</strong>：直接学习一个<strong>策略函数</strong>（Policy Function），输入状态，输出动作的概率分布。</li><li><strong>特点</strong>：可以处理连续动作空间，通常是随机性的（输出概率）。</li><li><strong>代表算法</strong>：REINFORCE (Policy Gradient)。</li></ul></li><li><strong>Actor-Critic（两者结合）</strong><ul><li><strong>原理</strong>：同时学习策略（Actor）和价值（Critic）。Actor 负责根据概率做出动作，Critic 负责根据价值函数对 Actor 的动作进行打分。</li><li><strong>代表算法</strong>：A3C, PPO, SAC。</li></ul></li></ul><h4 id="环境交互采集数据的策略与用以学习更新的策略是否一致进行分类">环境交互采集数据的策略与用以学习更新的策略是否一致进行分类</h4><p>这种分类方式关注的是“干活的”和“学习的”是不是同一个策略。</p><ul><li><strong>同轨策略（On-policy）</strong><ul><li><strong>原理</strong>：<strong>行为策略</strong>（用于采集数据）和<strong>目标策略</strong>（通过学习被更新的策略）是同一个。即“我学我刚才做的事情”。一旦策略更新了，之前的历史数据就失效了。</li><li><strong>特点</strong>：数据利用率较低，但稳定性通常较好。</li><li><strong>代表算法</strong>：Sarsa, PPO, TRPO。</li></ul></li><li><strong>离轨策略（Off-policy）</strong><ul><li><strong>原理</strong>：<strong>行为策略</strong>和<strong>目标策略</strong>是不同的。智能体可以将过去的经验存储在经验回放池（Experience Replay）中，从别人的经验或自己的历史经验中学习。</li><li><strong>特点</strong>：样本效率高（Sample Efficient），但收敛较难。</li><li><strong>代表算法</strong>：Q-Learning, DQN, SAC。</li></ul></li></ul><h4 id="是否学习环境的模型进行分类">是否学习环境的模型进行分类</h4><p>这种分类方式关注智能体是否试图在脑海中构建一个“世界模型”来模拟环境的运作。</p><ul><li><strong>无模型（Model-free）</strong><ul><li><strong>原理</strong>：不尝试理解环境的物理规律（即状态转移概率 <span class="math inline"><em>P</em>(<em>s</em><sup>′</sup>|<em>s</em>, <em>a</em>)</span>），直接通过大量的试错（Trial-and-Error）来学习策略或价值。</li><li><strong>特点</strong>：目前主流的 RL 方法，计算量相对较小，容易实现，但需要大量数据。</li><li><strong>代表算法</strong>：DQN, PPO。</li></ul></li><li><strong>有模型（Model-based）</strong><ul><li><strong>原理</strong>：智能体尝试学习一个<strong>环境模型</strong>（Model），预测采取动作后状态会如何变化以及奖励是多少。有了模型，智能体可以在脑海中进行“规划”（Planning）。</li><li><strong>特点</strong>：样本效率极高，但如果模型学得不准，会产生误差累积。</li><li><strong>代表算法</strong>：Dyna-Q, AlphaZero (利用 MCTS 进行规划), World Models。</li></ul></li></ul><h4 id="是否和环境进行交互进行分类">是否和环境进行交互进行分类</h4><p>这种分类方式关注数据来源是实时的还是历史固定的。</p><ul><li><strong>在线强化学习（Online RL）</strong><ul><li><strong>原理</strong>：智能体一边与环境交互，一边获取数据并更新模型。这是最经典的强化学习模式。</li><li><strong>特点</strong>：探索与利用的循环。</li></ul></li><li><strong>离线强化学习（Offline RL / Batch RL）</strong><ul><li><strong>原理</strong>：智能体不能与环境交互，只能从一个固定的、已经收集好的数据集（Dataset）中学习策略。这非常适用于医疗、自动驾驶等不能随意试错的领域。</li><li><strong>特点</strong>：主要挑战是分布偏移（Distribution Shift）问题。</li><li><strong>代表算法</strong>：CQL, BCQ。</li></ul></li></ul><h3 id="强化学习中的智能体和环境">强化学习中的智能体和环境</h3><p>强化学习的核心，在于<strong>智能体（Agent）</strong>与<strong>环境（Environment）</strong>之间持续不断的交互。这种交互是一个闭环过程，我们可以将其形象地比喻为“玩游戏”的过程。</p><h4 id="核心角色">核心角色</h4><ul><li><strong>智能体 (Agent)</strong><ul><li>它是强化学习的主角，是感知环境并做出决策的实体。</li><li><strong>功能</strong>：它观察当前的状态，根据内部的策略（Policy）选择一个动作，并试图通过这些动作来最大化未来的累积奖励。</li><li><em>例子</em>：在超级马里奥游戏中，控制马里奥移动和跳跃的程序就是智能体。</li></ul></li><li><strong>环境 (Environment)</strong><ul><li>它是智能体所处的外部世界，受自然规律或游戏规则支配。</li><li><strong>功能</strong>：它接收智能体的动作，更新自身的状态，并反馈给智能体一个奖励信号。</li><li><em>例子</em>：超级马里奥的游戏程序本身，包括地图、敌人、物理碰撞逻辑等。</li></ul></li></ul><h4 id="交互过程-the-interaction-loop">交互过程 (The Interaction Loop)</h4><p>智能体与环境的交互遵循一个离散的时间序列（<span class="math inline"><em>t</em> = 0, 1, 2, ...</span>），每一步交互包含以下三个关键要素：</p><ol type="1"><li><strong>状态 (State, <span class="math inline"><em>S</em><sub><em>t</em></sub></span>)</strong><ul><li>环境在 <span class="math inline"><em>t</em></span> 时刻呈现给智能体的样子。它可以是完全可观测的（如棋盘），也可以是部分可观测的（如迷宫视野）。</li></ul></li><li><strong>动作 (Action, <span class="math inline"><em>A</em><sub><em>t</em></sub></span>)</strong><ul><li>智能体基于当前状态 <span class="math inline"><em>S</em><sub><em>t</em></sub></span> 选择的具体行为。动作空间可以是离散的（如“上下左右”），也可以是连续的（如“方向盘转动角度”）。</li></ul></li><li><strong>奖励 (Reward, <span class="math inline"><em>R</em><sub><em>t</em> + 1</sub></span>)</strong><ul><li>智能体执行动作后，环境反馈的一个标量数值。</li><li><strong>本质</strong>：它是对动作好坏的即时评价。</li><li><em>注意</em>：奖励虽然是即时的，但智能体的目标是最大化<strong>长期累积奖励（Return）</strong>，而不仅仅是眼前的利益。</li></ul></li></ol><h4 id="形式化描述">形式化描述</h4><p>这一过程通常可以用数学公式表示为：</p><p><span class="math display">$$ S_t \xrightarrow{\text{Agent chooses } A_t} A_t \xrightarrow{\text{Environment returns}} (R_{t+1}, S_{t+1}) $$</span></p><p>其中：</p><ul><li><ol type="1"><li>在时刻 <span class="math inline"><em>t</em></span>，智能体观察到状态 <span class="math inline"><em>S</em><sub><em>t</em></sub></span>。</li></ol></li><li><ol start="2" type="1"><li>智能体采取动作 <span class="math inline"><em>A</em><sub><em>t</em></sub></span>。</li></ol></li><li><ol start="3" type="1"><li>环境根据物理规则发生变化，转移到新的状态 <span class="math inline"><em>S</em><sub><em>t</em> + 1</sub></span>，并给予智能体奖励 <span class="math inline"><em>R</em><sub><em>t</em> + 1</sub></span>。</li></ol></li></ul><p>这个循环不断进行，直到环境达到一个终止状态（Terminal State），例如游戏通关或游戏结束。</p><h3 id="强化学习中的回报和策略">强化学习中的回报和策略</h3><p>在了解了智能体和环境的交互后，我们需要定义智能体到底在追求什么（回报），以及它是如何做决定的（策略）。</p><h4 id="回报-return">回报 (Return)</h4><p>智能体的目标不是仅仅最大化当前的那个奖励 <span class="math inline"><em>R</em><sub><em>t</em></sub></span>，而是最大化<strong>长期累积奖励</strong>。我们将这个累积的奖励总和称为<strong>回报 (Return)</strong>，通常用 <span class="math inline"><em>G</em><sub><em>t</em></sub></span> 表示。</p><p>如果一个任务有明确的终点（Episodic Task），回报就是所有步数奖励的简单相加。但对于没有终点的持续性任务（Continuing Task），或者为了反映“现在的奖励比未来的奖励更值钱”这一概念，我们引入了<strong>折扣因子 (Discount Factor)</strong> <span class="math inline"><em>γ</em></span>。</p><ul><li><strong>折扣回报公式</strong>：</li></ul><p><span class="math display">$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$</span></p><ul><li><strong>折扣因子 (<span class="math inline"><em>γ</em></span>)</strong>：<ul><li>取值范围是 <span class="math inline">[0, 1]</span>。</li><li><strong><span class="math inline"><em>γ</em> = 0</span></strong>：智能体是“目光短浅”的，只在这个时刻关心即时奖励 <span class="math inline"><em>R</em><sub><em>t</em> + 1</sub></span>。</li><li><strong><span class="math inline"><em>γ</em> → 1</span></strong>：智能体是“高瞻远瞩”的，未来的奖励和当前的奖励几乎一样重要。</li><li><strong>作用</strong>：它不仅在数学上保证了无限步数下回报的收敛，也符合人类“落袋为安”的直觉。</li></ul></li></ul><h4 id="策略-policy">策略 (Policy)</h4><p>策略是智能体的“大脑”或“行动指南”。它定义了智能体在特定状态下应该采取什么动作。策略通常用符号 <span class="math inline"><em>π</em></span> 表示。</p><p>根据表现形式，策略主要分为两种：</p><ul><li><strong>确定性策略 (Deterministic Policy)</strong><ul><li><strong>定义</strong>：在某个状态 <span class="math inline"><em>s</em></span> 下，策略直接给出一个确定的动作 <span class="math inline"><em>a</em></span>。</li><li><strong>公式</strong>： <span class="math display"><em>a</em> = <em>π</em>(<em>s</em>)</span></li><li><strong>场景</strong>：适用于最优解唯一且环境稳定的情况。比如在简单的迷宫中，看到墙壁（状态）就向右转（动作）。</li></ul></li><li><strong>随机性策略 (Stochastic Policy)</strong><ul><li><strong>定义</strong>：在某个状态 <span class="math inline"><em>s</em></span> 下，策略给出的是所有可能动作的<strong>概率分布</strong>。智能体根据这个概率采样来决定执行哪个动作。</li><li><strong>公式</strong>： <span class="math display"><em>π</em>(<em>a</em>|<em>s</em>) = <em>P</em>(<em>A</em><sub><em>t</em></sub> = <em>a</em>|<em>S</em><sub><em>t</em></sub> = <em>s</em>)</span></li><li><strong>场景</strong>：<ol type="1"><li><strong>探索 (Exploration)</strong>：通过引入随机性，让智能体尝试不同的动作，避免陷入局部最优。</li><li><strong>博弈</strong>：在石头剪刀布游戏中，最好的策略是随机出拳，防止被对手预测。</li></ol></li></ul></li></ul><h3 id="强化学习中的价值函数">强化学习中的价值函数</h3><p>有了回报（Return）的概念后，我们面临一个实际问题：回报是在游戏结束或很长一段时间后才能统计出来的。但在做决策的当下，我们需要一个“预测器”来告诉我们：<strong>现在的局势有多好？</strong> 或者 <strong>这一步棋有多妙？</strong></p><p>这就是<strong>价值函数 (Value Function)</strong> 的作用。它是对未来累积回报的<strong>期望 (Expectation)</strong>。</p><h4 id="状态价值函数-state-value-function-v">状态价值函数 (State-Value Function, <span class="math inline"><em>V</em></span>)</h4><p>状态价值函数用于评估在特定策略 <span class="math inline"><em>π</em></span> 下，<strong>处于某个状态 <span class="math inline"><em>s</em></span> 到底有多好</strong>。</p><ul><li><p><strong>定义</strong>：从状态 <span class="math inline"><em>s</em></span> 开始，遵循策略 <span class="math inline"><em>π</em></span> 一直走下去，预期能得到的平均回报。</p></li><li><p><strong>数学公式</strong>： <span class="math display"><em>V</em><sub><em>π</em></sub>(<em>s</em>) = 𝔼<sub><em>π</em></sub>[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub> = <em>s</em>]</span></p></li><li><p><strong>直观理解</strong>：</p><ul><li>就像下围棋时，你要判断当前的“盘面”是优势还是劣势。即使还没走下一步，看着棋盘（状态 <span class="math inline"><em>s</em></span>），大师就能通过经验（价值函数）判断出胜率（价值 <span class="math inline"><em>V</em></span>）。</li></ul></li></ul><h4 id="动作价值函数-action-value-function-q">动作价值函数 (Action-Value Function, <span class="math inline"><em>Q</em></span>)</h4><p>动作价值函数不仅看状态，还看动作。它用于评估在特定策略 <span class="math inline"><em>π</em></span> 下，<strong>在状态 <span class="math inline"><em>s</em></span> 采取动作 <span class="math inline"><em>a</em></span> 到底有多好</strong>。这就是著名的 <strong>Q值 (Q-value)</strong>。</p><ul><li><p><strong>定义</strong>：从状态 <span class="math inline"><em>s</em></span> 开始，<strong>强制执行动作 <span class="math inline"><em>a</em></span></strong>，之后继续遵循策略 <span class="math inline"><em>π</em></span> 走下去，预期能得到的平均回报。</p></li><li><p><strong>数学公式</strong>： <span class="math display"><em>Q</em><sub><em>π</em></sub>(<em>s</em>, <em>a</em>) = 𝔼<sub><em>π</em></sub>[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub> = <em>s</em>, <em>A</em><sub><em>t</em></sub> = <em>a</em>]</span></p></li><li><p><strong>直观理解</strong>：</p><ul><li>这就好比在刚才的围棋盘面（状态 <span class="math inline"><em>s</em></span>）下，你正在考虑“落子天元”（动作 <span class="math inline"><em>a</em></span>）。Q函数会告诉你，这一步落下后，最终获胜的概率是多少。</li></ul></li><li><p><strong>重要性</strong>：在<strong>无模型 (Model-free)</strong> 强化学习中，Q函数尤为重要。因为如果我们知道每个动作的 Q值，我们只需要选择 Q值最大的那个动作（<span class="math inline">arg max<sub><em>a</em></sub><em>Q</em>(<em>s</em>, <em>a</em>)</span>），就是当前的最优决策，而不需要知道环境的具体物理规律。</p></li></ul><h4 id="两者的关系与贝尔曼方程-bellman-equation">两者的关系与贝尔曼方程 (Bellman Equation)</h4><p>价值函数不是静态的，它具有递归的性质。当前的价值，等于<strong>即时奖励</strong>加上<strong>打折后的未来状态价值</strong>。这就是著名的贝尔曼方程，它是强化学习进行迭代更新的核心。</p><ul><li><p><strong>V 与 Q 的关系</strong>： 状态 <span class="math inline"><em>s</em></span> 的价值，其实就是该状态下所有可能动作的 Q值，按照策略 <span class="math inline"><em>π</em></span> 的概率进行加权平均： <span class="math display"><em>V</em><sub><em>π</em></sub>(<em>s</em>) = ∑<sub><em>a</em></sub><em>π</em>(<em>a</em>|<em>s</em>)<em>Q</em><sub><em>π</em></sub>(<em>s</em>, <em>a</em>)</span></p></li><li><p><strong>贝尔曼方程的形态</strong>： <span class="math display"><em>V</em><sub><em>π</em></sub>(<em>s</em>) = 𝔼<sub><em>π</em></sub>[<em>R</em><sub><em>t</em> + 1</sub> + <em>γ</em><em>V</em><sub><em>π</em></sub>(<em>S</em><sub><em>t</em> + 1</sub>) ∣ <em>S</em><sub><em>t</em></sub> = <em>s</em>]</span></p></li></ul><p>这意味着：<strong>今天的价值 = 今天的奖励 + 明天的价值（打个折）</strong>。这种递归结构使得我们可以利用动态规划或时序差分（TD Learning）来一步步估算价值。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/" class="print-no-link">#Research</a> <a href="/tags/Algorithm/" class="print-no-link">#Algorithm</a></div></div><div class="license-box my-3"><div class="license-title"><div>Reinforcement Learning——RL系列目录</div><div>https://choucisan.xyz/RL.html</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>choucisan</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年12月8日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/Q-Learning.html" title="Q-Learning——一种基于值迭代的强化学习算法"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Q-Learning——一种基于值迭代的强化学习算法</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/BuildBlog.html" title="BlogShare——如何做好一个博客"><span class="hidden-mobile">BlogShare——如何做好一个博客</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",function(){var i=Object.assign({appId:"H13sJOhFUDSFRB5l6uGK3P1R-gzGzoHsz",appKey:"9PsUJ7XI7U3Pg3ixfxWfPD7B",path:"window.location.pathname",placeholder:"Say something / 说点什么吧~",avatar:"retro",meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"en",highlight:!1,recordIP:!1,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!1},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)})})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a></main><footer><div class="footer-inner"><div class="footer-content">© 2025 13 Lab. All Rights Reserved.</div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t=t.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback(function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())})</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/custom-logic.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>