<!DOCTYPE html><html lang="en,zh-CN"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="codeva-X99uCprQdB"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="choucisan"><meta name="keywords" content=""><meta name="description" content="引言： 直接偏好优化算法（DPO）是一种新的参数化方式，能够以闭式解的形式提取相应的最优策略，使仅通过简单的分类损失就能解决标准的RLHF问题。它稳定、高效且计算量小，无需在微调过程中从语言模型中进行采样，也不需要进行大量的超参数调整。我们的实验表明，DPO能够对语言模型进行微调，使其与人类偏好保持一致，效果达到甚至优于现有方法。值得注意的是，使用DPO进行微调在控制生成"><meta property="og:type" content="article"><meta property="og:title" content="DPO——直接偏好优化算法"><meta property="og:url" content="https://choucisan.xyz/zh-CN/DPO.html"><meta property="og:site_name" content="choucisan&#39;s blog"><meta property="og:description" content="引言： 直接偏好优化算法（DPO）是一种新的参数化方式，能够以闭式解的形式提取相应的最优策略，使仅通过简单的分类损失就能解决标准的RLHF问题。它稳定、高效且计算量小，无需在微调过程中从语言模型中进行采样，也不需要进行大量的超参数调整。我们的实验表明，DPO能够对语言模型进行微调，使其与人类偏好保持一致，效果达到甚至优于现有方法。值得注意的是，使用DPO进行微调在控制生成"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://images.unsplash.com/photo-1665559490709-bacaefc49b7b?q=80&w=2071&auto=format&fit=crop"><meta property="article:published_time" content="2025-12-21T09:58:52.000Z"><meta property="article:modified_time" content="2025-12-12T09:20:44.148Z"><meta property="article:author" content="choucisan"><meta property="article:tag" content="Research"><meta property="article:tag" content="Algorithm"><meta property="article:tag" content="DPO"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://images.unsplash.com/photo-1665559490709-bacaefc49b7b?q=80&w=2071&auto=format&fit=crop"><title>DPO——直接偏好优化算法 - choucisan&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link rel="stylesheet" href="/css/fullscreen-style.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"choucisan.xyz",root:"/",version:"1.9.8",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"VnqS06x2hxlj7MEjoUAix6L5-gzGzoHsz",app_key:"ck0fCo8cxJs0OdTEdWmSl9l1",server_url:"https://vnqs06x2.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2025-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>13log</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><span>HOME</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><span>ARCHIVES</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>TOPICS</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="/categories/Pub/" target="_self"><span>Publications</span> </a><a class="dropdown-item" href="/categories/OpenSource/" target="_self"><span>Open Source</span> </a><a class="dropdown-item" href="/categories/Paper/" target="_self"><span>Paper Reading</span> </a><a class="dropdown-item" href="/categories/Tech/" target="_self"><span>Tech Share</span> </a><a class="dropdown-item" href="/categories/Life/" target="_self"><span>Life Share</span></a></div></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><span>ABOUT</span></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">DPO——直接偏好优化算法</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-12-21 17:58" pubdate>2025年12月21日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.5k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 30 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">DPO——直接偏好优化算法</h1><div class="markdown-body"><div class="note note-info"><p><strong>引言：</strong> 直接偏好优化算法（DPO）是一种新的参数化方式，能够以闭式解的形式提取相应的最优策略，使仅通过简单的分类损失就能解决标准的RLHF问题。它稳定、高效且计算量小，无需在微调过程中从语言模型中进行采样，也不需要进行大量的超参数调整。我们的实验表明，DPO能够对语言模型进行微调，使其与人类偏好保持一致，效果达到甚至优于现有方法。值得注意的是，使用DPO进行微调在控制生成内容的情感方面超过了基于PPO的RLHF，并且在摘要和单轮对话任务中，在响应质量上与之相当或有所提升，同时实现和训练过程也简单得多。</p></div><h2 id="dpo算法介绍">✈️ DPO算法介绍</h2><h3 id="算法归类">算法归类</h3><p>DPO（Direct Preference Optimization）在强化学习与大模型微调的版图中，属于一种<strong>返璞归真</strong>的算法：</p><ul><li><strong>离线策略 (Off-Policy / Offline)</strong>：严格来说，DPO 更接近于监督学习。它不需要在训练过程中让模型实时生成数据（像 PPO 那样采样），而是直接使用预先构建好的偏好数据集（Triplets: Prompt, Winner, Loser）。</li><li><strong>基于策略 (Policy-Based)</strong>：它直接优化策略网络（LLM本身），没有显式的价值函数（Value Function）。</li><li><strong>无奖励模型 (Reward-Free)</strong>：这是 DPO 最显著的标签。它不需要训练一个独立的 Reward Model，而是通过数学变换，将“奖励”隐式地包含在策略模型中。</li></ul><h3 id="适用范围与局限">适用范围与局限</h3><ul><li><strong>适用范围</strong>：<ul><li><strong>RLHF 替代方案</strong>：适用于所有需要人类偏好对齐的场景，如聊天机器人（Chatbot）、文本摘要、指令跟随等。</li><li><strong>资源受限场景</strong>：由于不需要加载 Critic 和 Reward Model，DPO 的显存占用约为 PPO 的一半，非常适合在有限显存下微调大模型。</li></ul></li><li><strong>局限性</strong>：<ul><li><strong>数据分布偏移 (Out-of-Distribution)</strong>：DPO 属于离线学习，如果偏好数据集的分布与模型实际生成的分布差异过大，效果可能会打折扣。</li><li><strong>缺乏探索 (Lack of Exploration)</strong>：PPO 在训练中会生成新数据进行探索，而 DPO 仅基于给定数据。对于需要极强逻辑推理或解空间很大的任务（如数学证明），DPO 可能不如在线 RL 方法（如 PPO 或 GRPO）。</li></ul></li></ul><h3 id="基本思想">基本思想</h3><p>DPO 的核心洞察极其精妙：<strong>“奖励函数 <span class="math inline"><em>r</em></span> 和最优策略 <span class="math inline"><em>π</em><sup>*</sup></span> 是一体两面的。”</strong></p><p>在 PPO 中，我们先训练一个奖励模型 <span class="math inline"><em>r</em>(<em>x</em>, <em>y</em>)</span>，然后训练策略 <span class="math inline"><em>π</em></span> 去最大化这个 <span class="math inline"><em>r</em></span>。 DPO 的作者发现，对于一个特定的奖励函数，最优策略 <span class="math inline"><em>π</em><sup>*</sup></span> 的解析解是可以写出来的。既然如此，我们为什么不反过来，<strong>用策略 <span class="math inline"><em>π</em></span> 来表示奖励 <span class="math inline"><em>r</em></span> 呢？</strong></p><p>这样，我们就可以把 RLHF 中那个“基于奖励模型的 Bradley-Terry 偏好概率”，直接转化为“基于策略概率的分类问题”。</p><hr><h2 id="dpo算法流程">🚀 DPO算法流程</h2><p>DPO 的核心优势在于将复杂的强化学习过程简化为直观的<strong>最大似然优化</strong>过程。它不再需要经验回放池（Replay Buffer）和复杂的采样（Sampling），而是直接在静态数据上进行计算。</p><figure><img src="/images/rl/dpo.webp" srcset="/img/loading.gif" lazyload alt="DPO算法流程图"><figcaption aria-hidden="true">DPO算法流程图</figcaption></figure><h3 id="dpo算法推导">DPO算法推导</h3><p>这是 DPO 最核心的数学魔法。我们分三步走：</p><p><strong>RLHF 的目标函数</strong> 标准的 RLHF 目标是最大化奖励，同时通过 KL 散度约束不偏离参考模型 <span class="math inline"><em>π</em><sub><em>r</em><em>e</em><em>f</em></sub></span>： <span class="math display">$$ \max_{\pi} \mathbb{E}_{x \sim D, y \sim \pi} [r(x,y) - \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)}] $$</span></p><p><strong>最优策略的解析解</strong> 通过变分法求解上述最优化问题，我们可以得到<strong>最优策略 <span class="math inline"><em>π</em><sup>*</sup></span> 的闭式解</strong>： <span class="math display">$$ \pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{r(x,y)}{\beta}\right) $$</span> 其中 <span class="math inline"><em>Z</em>(<em>x</em>)</span> 是归一化因子。</p><p><strong>反解奖励函数 (The Magic)</strong> 现在，我们对上面的公式进行移项，把奖励函数 <span class="math inline"><em>r</em>(<em>x</em>, <em>y</em>)</span> 单独解出来： <span class="math display">$$ r(x,y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x) $$</span> <strong>这个公式不仅消除了显式的奖励模型，还把奖励直接挂钩到了策略概率上！</strong></p><p><strong>代入 Bradley-Terry 模型</strong> Bradley-Terry 模型定义了偏好概率： <span class="math display"><em>P</em>(<em>y</em><sub><em>w</em></sub> &gt; <em>y</em><sub><em>l</em></sub>) = <em>σ</em>(<em>r</em>(<em>x</em>, <em>y</em><sub><em>w</em></sub>) − <em>r</em>(<em>x</em>, <em>y</em><sub><em>l</em></sub>))</span> 我们将第三步反解出的 <span class="math inline"><em>r</em>(<em>x</em>, <em>y</em>)</span> 代入上式（注意 <span class="math inline"><em>Z</em>(<em>x</em>)</span> 会被消掉），最终得到了 <strong>DPO 的损失函数</strong>：</p><p><span class="math display">$$ L_{DPO}(\pi_\theta; \pi_{ref}) = - \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right] $$</span></p><h3 id="dpo算法实现流程">DPO算法实现流程</h3><p>DPO 的实际训练过程类似于监督学习（Supervised Learning），但需要同时维护两个模型进行前向传播。以下是单步更新的完整逻辑流程：</p><ul><li><strong>输入准备 (Input Preparation)</strong><ul><li>从偏好数据集 <span class="math inline"><em>D</em></span> 中采样一个批次的数据。</li><li>每条数据包含三部分：提示词 <span class="math inline"><em>x</em></span> (Prompt)，人类偏好的回答 <span class="math inline"><em>y</em><sub><em>w</em></sub></span> (Winner)，人类排斥的回答 <span class="math inline"><em>y</em><sub><em>l</em></sub></span> (Loser)。</li></ul></li><li><strong>参考模型前向传播 (Reference Forward)</strong><ul><li>使用<strong>参考模型</strong>（冻结参数，不更新）分别计算 <span class="math inline"><em>y</em><sub><em>w</em></sub></span> 和 <span class="math inline"><em>y</em><sub><em>l</em></sub></span> 的对数概率。</li><li>输出：<span class="math inline">log <em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>(<em>y</em><sub><em>w</em></sub>|<em>x</em>)</span> 和 <span class="math inline">log <em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>(<em>y</em><sub><em>l</em></sub>|<em>x</em>)</span>。</li></ul></li><li><strong>策略模型前向传播 (Policy Forward)</strong><ul><li>使用<strong>当前策略模型</strong>（需要更新参数）分别计算 <span class="math inline"><em>y</em><sub><em>w</em></sub></span> 和 <span class="math inline"><em>y</em><sub><em>l</em></sub></span> 的对数概率。</li><li>输出：<span class="math inline">log <em>π</em><sub><em>θ</em></sub>(<em>y</em><sub><em>w</em></sub>|<em>x</em>)</span> 和 <span class="math inline">log <em>π</em><sub><em>θ</em></sub>(<em>y</em><sub><em>l</em></sub>|<em>x</em>)</span>。</li></ul></li><li><strong>计算隐式奖励 (Implicit Reward Calculation)</strong><ul><li>计算两个模型对“胜出回答”的概率差值（Log Ratio）：<span class="math inline"><em>R</em><sub><em>w</em></sub> = <em>β</em>(log <em>π</em><sub><em>θ</em></sub>(<em>y</em><sub><em>w</em></sub>|<em>x</em>) − log <em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>(<em>y</em><sub><em>w</em></sub>|<em>x</em>))</span></li><li>计算两个模型对“落败回答”的概率差值（Log Ratio）：<span class="math inline"><em>R</em><sub><em>l</em></sub> = <em>β</em>(log <em>π</em><sub><em>θ</em></sub>(<em>y</em><sub><em>l</em></sub>|<em>x</em>) − log <em>π</em><sub><em>r</em><em>e</em><em>f</em></sub>(<em>y</em><sub><em>l</em></sub>|<em>x</em>))</span></li><li><em>注：这里的 <span class="math inline"><em>R</em><sub><em>w</em></sub></span> 和 <span class="math inline"><em>R</em><sub><em>l</em></sub></span> 实际上代表了模型相对于参考模型获得的“隐式奖励”。</em></li></ul></li><li><strong>计算损失与反向传播 (Loss &amp; Backward)</strong><ul><li>计算两者的奖励差值（Margin）：<span class="math inline"><em>M</em><em>a</em><em>r</em><em>g</em><em>i</em><em>n</em> = <em>R</em><sub><em>w</em></sub> − <em>R</em><sub><em>l</em></sub></span>。</li><li>将差值通过 Sigmoid 函数并取负对数，得到分类损失：<span class="math inline"><em>L</em><em>o</em><em>s</em><em>s</em> = −log <em>σ</em>(<em>M</em><em>a</em><em>r</em><em>g</em><em>i</em><em>n</em>)</span>。</li><li><strong>直观理解</strong>：我们希望 <span class="math inline"><em>R</em><sub><em>w</em></sub></span> 越大越好，<span class="math inline"><em>R</em><sub><em>l</em></sub></span> 越小越好，即拉大胜者和败者之间的距离。</li><li>根据 Loss 计算梯度，更新<strong>当前策略模型</strong>的参数 <span class="math inline"><em>θ</em></span>。</li></ul></li></ul><p>通过不断循环上述步骤，策略模型 <span class="math inline"><em>π</em><sub><em>θ</em></sub></span> 会逐渐“远离”参考模型对差答案的生成概率，并“靠近”参考模型对好答案的生成概率（同时加入人类偏好的修正）。</p><h2 id="dpo的理论分析">🔥 DPO的理论分析</h2><h3 id="llm与奖励模型">LLM与奖励模型</h3><p>在 PPO 时代，我们需要训练一个显式的 <strong>Reward Model (RM)</strong> 来充当“判卷老师”，告诉策略模型哪句话写得好。但 DPO 的数学推导揭示了一个深刻的理论事实：</p><p><strong>语言模型本身就隐含了一个奖励模型。</strong></p><ul><li><strong>冗余性</strong>：当我们训练 LLM 去逼近最优策略 <span class="math inline"><em>π</em><sup>*</sup></span> 时，策略模型的概率分布变化 <span class="math inline">$\frac{\pi(y|x)}{\pi_{ref}(y|x)}$</span> 其实就是在隐式地表达它对回复的偏好。</li><li><strong>去中心化</strong>：因此，维护一个独立的 Reward Model 其实是<strong>冗余</strong>的，甚至可能因为 RM 自身的偏差引入额外的拟合误差。DPO 直接优化策略，相当于<strong>去掉了“中间商”</strong>，让偏好数据直接指导语言模型的生成概率。</li></ul><h3 id="actor-critic-algorithms-的不稳定性">Actor-Critic Algorithms 的不稳定性</h3><p>为什么 PPO 这种 Actor-Critic 算法在大模型微调中如此难训？根本原因在于架构内部的<strong>复杂动态博弈</strong>：</p><ul><li><strong>移动的靶子 (Moving Target)</strong>：<ul><li>Actor 依赖 Critic 的打分来更新参数。</li><li>Critic 依赖 Actor 采样的数据来更新参数。</li><li>只要其中一方稍微“跑偏”，另一方就会跟着学坏，导致训练震荡甚至崩塌。</li></ul></li><li><strong>复杂的超参数协调</strong>：<ul><li>PPO 需要同时维护 Actor、Critic、Reward Model、Reference Model 四个网络。</li><li>需要精细调节 GAE 系数 <span class="math inline"><em>λ</em></span>、截断系数 <span class="math inline"><em>ϵ</em></span>、价值损失系数 <span class="math inline"><em>c</em><sub>1</sub></span>、熵系数 <span class="math inline"><em>c</em><sub>2</sub></span> 等，牵一发而动全身。</li></ul></li></ul><p><strong>DPO 的稳定性优势</strong>： DPO 将复杂的强化学习问题转化为了一个简单的<strong>二分类监督学习问题</strong>。</p><ul><li><strong>无 Critic</strong>：消除了价值估计带来的偏差和方差。</li><li><strong>无采样</strong>：消除了生成过程中的随机性。</li><li><strong>凸优化性质</strong>：DPO 的目标函数性质更好（接近凸函数），这使得训练过程像普通的 SFT（监督微调）一样丝滑稳定，极大地降低了 RLHF 的门槛。</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/" class="print-no-link">#Research</a> <a href="/tags/Algorithm/" class="print-no-link">#Algorithm</a> <a href="/tags/DPO/" class="print-no-link">#DPO</a></div></div><div class="license-box my-3"><div class="license-title"><div>DPO——直接偏好优化算法</div><div>https://choucisan.xyz/DPO.html</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>choucisan</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年12月21日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/GRPO.html" title="GRPO——组相对策略优化算法"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">GRPO——组相对策略优化算法</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/PPO.html" title="PPO——近端策略优化算法"><span class="hidden-mobile">PPO——近端策略优化算法</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://lib.baomitu.com/valine/1.5.1/Valine.min.js",function(){var i=Object.assign({appId:"H13sJOhFUDSFRB5l6uGK3P1R-gzGzoHsz",appKey:"9PsUJ7XI7U3Pg3ixfxWfPD7B",path:"window.location.pathname",placeholder:"Say something / 说点什么吧~",avatar:"retro",meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"en",highlight:!1,recordIP:!1,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!1},{el:"#valine",path:window.location.pathname});new Valine(i),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{var i="#valine .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)})})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a></main><footer><div class="footer-inner"><div class="footer-content">© 2025 13 Lab. All Rights Reserved.</div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t=t.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback(function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())})</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/custom-logic.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>