<!DOCTYPE html><html lang="en,zh-CN"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="codeva-X99uCprQdB"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="choucisan"><meta name="keywords" content=""><meta name="description" content="想象力比知识更重要！我会在这里分享我的公开发表物、论文阅读、开源项目、技术分享、生活分享，欢迎大家关注！我会持续更新！"><meta property="og:type" content="website"><meta property="og:title" content="choucisan&#39;s blog"><meta property="og:url" content="https://choucisan.xyz/index.html"><meta property="og:site_name" content="choucisan&#39;s blog"><meta property="og:description" content="想象力比知识更重要！我会在这里分享我的公开发表物、论文阅读、开源项目、技术分享、生活分享，欢迎大家关注！我会持续更新！"><meta property="og:locale" content="en_US"><meta property="article:author" content="choucisan"><meta name="twitter:card" content="summary_large_image"><title>choucisan&#39;s blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link rel="stylesheet" href="/css/fullscreen-style.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"choucisan.xyz",root:"/",version:"1.9.8",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:"VnqS06x2hxlj7MEjoUAix6L5-gzGzoHsz",app_key:"ck0fCo8cxJs0OdTEdWmSl9l1",server_url:"https://vnqs06x2.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2025-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:100vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>13log</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><span>HOME</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><span>ARCHIVES</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>TOPICS</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="/categories/Pub/" target="_self"><span>Publications</span> </a><a class="dropdown-item" href="/categories/OpenSource/" target="_self"><span>Open Source</span> </a><a class="dropdown-item" href="/categories/Paper/" target="_self"><span>Paper Reading</span> </a><a class="dropdown-item" href="/categories/Tech/" target="_self"><span>Tech Share</span> </a><a class="dropdown-item" href="/categories/Life/" target="_self"><span>Life Share</span></a></div></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><span>ABOUT</span></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle"></span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></div></header><main><div class="container nopadding-x-md"><div id="board" style="margin-top:0"><div class="container"><div class="row"><div class="col-12 col-md-10 m-auto"><h1 style="display:none">choucisan&#39;s blog</h1><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/GRPO.html" target="_self"><img src="https://images.unsplash.com/photo-1603467591753-663ce916a757?q=80&amp;w=2072&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="GRPO——组相对策略优化算法"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/GRPO.html" target="_self">GRPO——组相对策略优化算法</a></h2><a class="index-excerpt" href="/GRPO.html" target="_self"><div>引言： 群体相对策略优化（GRPO）是由 DeepSeek 团队提出的一种专为大语言模型设计的高效强化学习算法。传统的 PPO 算法虽然强大，但需要维护一个与策略模型同等规模的价值网络（Critic），这对显存带来了巨大的压力。GRPO 创造性地摒弃了 Critic 网络，转而利用群体采样的统计特征来估计优势函数。这种方法不仅大幅降低了训练时的显存占用和计算成本，还在数学</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-25 17:58" pubdate>2025-12-25</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a> <a href="/tags/GRPO/">#GRPO</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/DPO.html" target="_self"><img src="https://images.unsplash.com/photo-1665559490709-bacaefc49b7b?q=80&amp;w=2071&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="DPO——直接偏好优化算法"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/DPO.html" target="_self">DPO——直接偏好优化算法</a></h2><a class="index-excerpt" href="/DPO.html" target="_self"><div>引言： 直接偏好优化算法（DPO）是一种新的参数化方式，能够以闭式解的形式提取相应的最优策略，使仅通过简单的分类损失就能解决标准的RLHF问题。它稳定、高效且计算量小，无需在微调过程中从语言模型中进行采样，也不需要进行大量的超参数调整。我们的实验表明，DPO能够对语言模型进行微调，使其与人类偏好保持一致，效果达到甚至优于现有方法。值得注意的是，使用DPO进行微调在控制生成</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-21 17:58" pubdate>2025-12-21</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a> <a href="/tags/DPO/">#DPO</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/PPO.html" target="_self"><img src="https://images.unsplash.com/photo-1742382717882-3cfb6971f44b?q=80&amp;w=2069&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="PPO——近端策略优化算法"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/PPO.html" target="_self">PPO——近端策略优化算法</a></h2><a class="index-excerpt" href="/PPO.html" target="_self"><div>引言： 近端策略优化（PPO）是一种新的强化学习策略梯度方法，该方法通过与环境交互采样数据和使用随机梯度上升优化“替代”目标函数来交替进行。标准的策略梯度方法每个数据样本执行一次梯度更新，PPO提出了一种新的目标函数能够实现多轮小批量更新，它具备信任区域策略优化（TRPO）的部分优势，但实现起来更为简单、适用性更广，并且（从经验上看）具有更好的样本复杂度。实验结果表明PP</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-15 11:00" pubdate>2025-12-15</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a> <a href="/tags/PPO/">#PPO</a> <a href="/tags/OpenAI/">#OpenAI</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/AlphaGomoku.html" target="_self"><img src="https://images.unsplash.com/photo-1660730288086-a305d9a1eab3?q=80&amp;w=1974&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="AlphaGomoku——MCTS算法与五子棋"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/AlphaGomoku.html" target="_self">AlphaGomoku——MCTS算法与五子棋</a></h2><a class="index-excerpt" href="/AlphaGomoku.html" target="_self"><div>引言： 蒙特卡洛搜索树（Monte Carlo Tree Search, MCTS）是一种用于决策的搜索算法，广泛应用于游戏、人工智能等领域。本文将介绍 MCTS 的基本原理，并通过一个五子棋的项目来展示如何使用 MCTS 算法进行游戏决策。 📖 蒙特卡洛搜索树（MCTS） MCTS是什么？ MCTS是一种用于决策过程的启发式搜索算法，最早在20</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-14 21:57" pubdate>2025-12-14</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a> <a href="/tags/Project/">#Project</a> <a href="/tags/MCTS/">#MCTS</a> <a href="/tags/AlphaGo/">#AlphaGo</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/DQN.html" target="_self"><img src="https://images.unsplash.com/photo-1530880587016-3b71d746ab6c?q=80&amp;w=2070&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="DQN——Deep Q-Learning算法"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/DQN.html" target="_self">DQN——Deep Q-Learning算法</a></h2><a class="index-excerpt" href="/DQN.html" target="_self"><div>引言： DQN（Deep Q-Network）是一种基于深度学习的强化学习算法，它通过构建一个神经网络来估计状态-动作值函数（Q函数），从而实现智能体的决策过程。DQN算法的核心思想是通过不断与环境交互，学习到最优的Q函数，从而找到最优的策略。本文将介绍DQN算法的基本原理、实现方法。 ✈️ DQN算法介绍 算法归类 从分类体系上看，DQN 完美继</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-10 17:58" pubdate>2025-12-10</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a> <a href="/tags/DQN/">#DQN</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/Q-Learning.html" target="_self"><img src="https://images.unsplash.com/photo-1698714038280-a682e9a6bf40?q=80&amp;w=2070&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="Q-Learning——一种基于值迭代的强化学习算法"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/Q-Learning.html" target="_self">Q-Learning——一种基于值迭代的强化学习算法</a></h2><a class="index-excerpt" href="/Q-Learning.html" target="_self"><div>引言： Q-Learning是一种基于值迭代的强化学习算法，它通过不断更新Q值来优化策略。Q值表示在给定状态下采取某个动作的预期回报。Q-Learning算法通过最大化Q值来选择最优动作，从而实现最优策略的求解,下面有是Q-Learning算法介绍与井字棋游戏的实现。 ✈️ Q-Learning算法介绍 算法归类 从分类上来看，Q-Learning</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-09 17:58" pubdate>2025-12-09</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a> <a href="/tags/Q-learning/">#Q-learning</a> <a href="/tags/TicTacToe/">#TicTacToe</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/RL.html" target="_self"><img src="https://images.unsplash.com/photo-1721815885269-b6ba1004f0a5?q=80&amp;w=2070&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="Reinforcement Learning——RL系列目录"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/RL.html" target="_self">Reinforcement Learning——RL系列目录</a></h2><a class="index-excerpt" href="/RL.html" target="_self"><div>引言： 最近在研究强化学习，所以打算写一系列关于强化学习的文章(实时更新)，包括Q-learning、DQN、MCTS、PPO、DPO、RLHF、GPRO等，这里是一些基础知识和后续文章的目录，可以在这里直接跳转你感兴趣的内容。 📖 RL系列文章目录 以后有关于强化学习的算法与文章都会在这里更新，目录如下： 1.Q-Learning——一种基于值</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-08 17:58" pubdate>2025-12-08</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Paper/" class="category-chain-item">Paper</a> <span>></span> <a href="/categories/Paper/Reinforcement-Learning/" class="category-chain-item">Reinforcement Learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Algorithm/">#Algorithm</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/BuildBlog.html" target="_self"><img src="https://images.unsplash.com/photo-1757367965959-2d9da79ad5d7?q=80&amp;w=2071&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="BlogShare——如何做好一个博客"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/BuildBlog.html" target="_self">BlogShare——如何做好一个博客</a></h2><a class="index-excerpt" href="/BuildBlog.html" target="_self"><div>引言： 做一个好的博客对学习积累、知识分享、个人能力提升都有很大的帮助。同时也是监督自己不断进步的一种方式。本文将分享互联网上优秀的博客网站(实时更新)，以及如何搭建一个属于自己的博客。 🌐优秀博客网站分享 技术大咖类 这里分享一些在技术领域非常优秀的博客网站，适合各个技术领域的人阅读。 阮一峰的网络日志 阮一峰老师是中文技术文章写作领域的大佬</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-05 00:39" pubdate>2025-12-05</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Tech/" class="category-chain-item">Tech</a> <span>></span> <a href="/categories/Tech/Info/" class="category-chain-item">Info</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Blog/">#Blog</a> <a href="/tags/Tool/">#Tool</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/GetPaperInfo.html" target="_self"><img src="https://images.unsplash.com/photo-1763959172796-4b09f0031e85?q=80&amp;w=1925&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="GetPaperInfo——如何高效获取文信息"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/GetPaperInfo.html" target="_self">GetPaperInfo——如何高效获取文信息</a></h2><a class="index-excerpt" href="/GetPaperInfo.html" target="_self"><div>引言： 在信息爆炸的今天，Research 的能力对于每个人都越来越重要。无论是为了学术研究，还是为了紧跟技术前沿，高效获取论文信息都是一项核心技能。本文整理了一套从“找论文”到“读论文”再到“审论文”的宝藏工具流，建议收藏备用！(实时更新) 🎯 我们要找哪些论文？ 在打开搜索引擎之前，我们需要明确目标。不同的研究阶段，我们需要关注不同类型的论文</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-03 11:42" pubdate>2025-12-03</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Tech/" class="category-chain-item">Tech</a> <span>></span> <a href="/categories/Tech/Info/" class="category-chain-item">Info</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Research/">#Research</a> <a href="/tags/Tools/">#Tools</a> <a href="/tags/Productivity/">#Productivity</a></div></div></article></div><div class="row mx-auto index-card"><div class="col-12 col-md-4 m-auto index-img"><a href="/hello-world.html" target="_self"><img src="https://images.unsplash.com/photo-1764377977361-af5592eb0531?q=80&amp;w=2070&amp;auto=format&amp;fit=crop" srcset="/img/loading.gif" lazyload alt="Hello World ！"></a></div><article class="col-12 col-md-8 mx-auto index-info"><h2 class="index-header"><a href="/hello-world.html" target="_self">Hello World ！</a></h2><a class="index-excerpt" href="/hello-world.html" target="_self"><div>引言： 这篇文章是我的第一篇博客，介绍博客定位、功能及示例。 本博客是使用 Hexo 框架搭建的，主题使用的是 Fluid ，同时参考了文章 Hexo配置与扩展 ，在此一同致谢所有开源作者！ 博客介绍 在这里我会介绍博客的一些功能，比如公开发表物、开源项目、论文阅读、技术分享、生活记录等，我会持续更新，欢迎关注！ 公开发表物 这里以后会发布一些公开</div></a><div class="index-btm post-metas"><div class="post-meta mr-3"><i class="iconfont icon-date"></i> <time datetime="2025-12-01 22:00" pubdate>2025-12-01</time></div><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Tech/" class="category-chain-item">Tech</a> <span>></span> <a href="/categories/Tech/Web/" class="category-chain-item">Web</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/blog/">#blog</a></div></div></article></div></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a></main><footer><div class="footer-inner"><div class="footer-content">© 2025 13 Lab. All Rights Reserved.</div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">Views: <span id="leancloud-site-pv"></span> </span><span id="leancloud-site-uv-container" style="display:none">Visitors: <span id="leancloud-site-uv"></span></span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/custom-logic.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>